{
 "cells": [
  {
   "cell_type": "code",
   "source": "\"\"\"\nå¿«é€Ÿæµ‹è¯• Admissible æ¨¡å¼æ˜¯å¦å¯ç”¨\n\"\"\"\n\nprint(\"=\"*60)\nprint(\"ğŸ§ª æµ‹è¯• Admissible æ¨¡å¼\")\nprint(\"=\"*60)\n\n# æµ‹è¯•å¯¼å…¥\nprint(\"\\n[1/2] æµ‹è¯•å¯¼å…¥ alfworld_trial_admissible.py...\")\ntry:\n    from alfworld_trial_admissible import llm_choose_from_admissible, alfworld_run, run_trial\n    print(\"âœ“ å¯¼å…¥æˆåŠŸ\")\n    print(f\"  - run_trial å‡½æ•°å¯ç”¨\")\n    print(f\"  - alfworld_run å‡½æ•°å¯ç”¨\")\n    print(f\"  - llm_choose_from_admissible å‡½æ•°å¯ç”¨\")\nexcept Exception as e:\n    print(f\"âœ— å¯¼å…¥å¤±è´¥: {e}\")\n    import traceback\n    traceback.print_exc()\n\n# éªŒè¯ç¯å¢ƒæ”¯æŒ admissible_commands\nprint(\"\\n[2/2] éªŒè¯ç¯å¢ƒæä¾› admissible_commands...\")\nimport yaml\nimport alfworld.agents.environment\n\nwith open('base_config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nenv = alfworld.agents.environment.get_environment(config['env']['type'])(config, train_eval='eval_out_of_distribution')\nenv = env.init_env(batch_size=1)\nob, info = env.reset()\n\nif 'admissible_commands' in info and len(info['admissible_commands']) > 0:\n    admissible = info['admissible_commands'][0]\n    print(f\"âœ“ ç¯å¢ƒæä¾› {len(admissible)} ä¸ªå€™é€‰åŠ¨ä½œ\")\n    print(f\"  ç¤ºä¾‹: {admissible[0]}\")\nelse:\n    print(\"âœ— ç¯å¢ƒä¸æä¾› admissible_commands\")\n\nenv.close()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ“ Admissible æ¨¡å¼æµ‹è¯•å®Œæˆ\")\nprint(\"=\"*60)\nprint(\"\\nğŸ“ ä½¿ç”¨æ–¹æ³•:\")\nprint(\"  åœ¨ cell-29 ä¸­ä¿®æ”¹:\")\nprint(\"  from alfworld_trial_admissible import run_trial\")\nprint(\"  ç„¶åè¿è¡Œå®éªŒå³å¯\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\næµ‹è¯•å€™é€‰åŠ¨ä½œé€‰æ‹©æ¨¡å¼\n\"\"\"\n\nprint(\"æµ‹è¯•å€™é€‰åŠ¨ä½œé€‰æ‹©æ¨¡å¼...\\n\")\n\nimport yaml\nimport importlib\nimport alfworld\nimport alfworld.agents.environment\n\n# åŠ è½½é…ç½®\nwith open('base_config.yaml') as f:\n    config = yaml.safe_load(f)\n\nsplit = \"eval_out_of_distribution\"\nenv_type = config[\"env\"][\"type\"]\n\n# åˆ›å»ºç¯å¢ƒ\nenv = alfworld.agents.environment.get_environment(env_type)(config, train_eval=split)\nenv = env.init_env(batch_size=1)\n\n# é‡ç½®\nob, info = env.reset()\nob_text = '\\n'.join(ob[0].split('\\n\\n')[1:])\nname = '/'.join(info['extra.gamefile'][0].split('/')[-3:-1])\n\nprint(f\"ä»»åŠ¡: {name}\\n\")\n\n# æ£€æŸ¥æ˜¯å¦æœ‰admissible commands\nif 'admissible_commands' in info:\n    admissible = info['admissible_commands'][0]\n    print(f\"âœ“ å€™é€‰åŠ¨ä½œæ•°: {len(admissible)}\")\n    print(f\"  ç¤ºä¾‹:\")\n    for i, cmd in enumerate(admissible[:5]):\n        print(f\"    {i+1}. {cmd}\")\nelse:\n    print(\"âš ï¸ æ²¡æœ‰admissible_commands\")\n\n# åŠ è½½å¯¹åº”çš„prompt\nPREFIXES = {\n    'pick_and_place': 'put',\n    'pick_clean_then_place': 'clean',\n    'pick_heat_then_place': 'heat',\n    'pick_cool_then_place': 'cool',\n    'look_at_obj': 'examine',\n    'pick_two_obj': 'puttwo'\n}\n\n# æ‰¾åˆ°å¯¹åº”çš„prompt\nprompt_key = None\nfor k, v in PREFIXES.items():\n    if name.startswith(k):\n        prompt_key = v\n        break\n\nif prompt_key:\n    with open('prompts/alfworld_3prompts.json', 'r') as f:\n        d = json.load(f)\n    base_prompt = 'Interact with a household to solve a task. Here are two examples.\\n' + d[f'react_{prompt_key}_1'] + d[f'react_{prompt_key}_0']\n    print(f\"\\nâœ“ ä½¿ç”¨prompt: react_{prompt_key}\")\nelse:\n    print(\"\\nâš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„prompt\")\n    base_prompt = \"Solve the household task.\"\n\n# æµ‹è¯•1: æµ‹è¯•åŠ¨ä½œé€‰æ‹©å‡½æ•°\nprint(\"\\n\" + \"=\"*60)\nprint(\"æµ‹è¯•1: LLMåŠ¨ä½œé€‰æ‹©\")\nprint(\"=\"*60)\n\nif 'admissible_commands' in info and len(info['admissible_commands']) > 0:\n    test_prompt = base_prompt + f\"\\n\\n{ob_text}\\n\\nWhat should I do first?\"\n    test_admissible = info['admissible_commands'][0][:10]  # åªæµ‹è¯•å‰10ä¸ª\n    \n    print(f\"\\nå€™é€‰åŠ¨ä½œ (å‰10ä¸ª):\")\n    for i, cmd in enumerate(test_admissible):\n        print(f\"  {i+1}. {cmd}\")\n    \n    print(f\"\\nè¯·LLMé€‰æ‹©...\")\n    selected = llm_choose_action(test_prompt, test_admissible, model=MODEL, temperature=0.0)\n    print(f\"âœ“ é€‰æ‹©: {selected}\")\n    \n    if selected in test_admissible:\n        print(\"âœ“ é€‰æ‹©æœ‰æ•ˆ\")\n    else:\n        print(\"âš ï¸ é€‰æ‹©æ— æ•ˆ\")\nelse:\n    print(\"âš ï¸ æ— æ³•æµ‹è¯• - æ²¡æœ‰å€™é€‰åŠ¨ä½œ\")\n\n# æµ‹è¯•2: è¿è¡Œå®Œæ•´episode (åªè¿è¡Œ5æ­¥)\nprint(\"\\n\" + \"=\"*60)\nprint(\"æµ‹è¯•2: è¿è¡Œ5æ­¥\")\nprint(\"=\"*60)\n\n# é‡ç½®ç¯å¢ƒ\nob, info = env.reset()\nob_text = '\\n'.join(ob[0].split('\\n\\n')[1:])\n\nenv_history = EnvironmentHistoryChoice(base_prompt, ob_text, [])\nenv_history.reset()\n\nprint(f\"\\nåˆå§‹è§‚å¯Ÿ:\")\nprint(ob_text[:200] + \"...\")\n\nfor step in range(5):\n    print(f\"\\n--- æ­¥éª¤ {step+1} ---\")\n    \n    if 'admissible_commands' in info and len(info['admissible_commands']) > 0:\n        admissible = info['admissible_commands'][0]\n        print(f\"å€™é€‰åŠ¨ä½œæ•°: {len(admissible)}\")\n        \n        # é€‰æ‹©åŠ¨ä½œ\n        action = llm_choose_action(\n            str(env_history),\n            admissible,\n            model=MODEL,\n            temperature=0.0\n        )\n    else:\n        # ç”Ÿæˆæ¨¡å¼\n        action = llm_simple(str(env_history) + \"\\n>\", stop=['\\n'], model=MODEL).strip()\n    \n    print(f\"åŠ¨ä½œ: {action}\")\n    env_history.add(\"action\", action)\n    \n    # æ‰§è¡Œ\n    observation, reward, done, info = env.step([action])\n    observation = process_ob(observation[0])\n    \n    if action.startswith('think:'):\n        observation = 'OK.'\n    \n    env_history.add(\"observation\", observation)\n    print(f\"è§‚å¯Ÿ: {observation[:100]}...\")\n    \n    if done:\n        print(f\"\\nâœ“ ä»»åŠ¡å®Œæˆï¼\")\n        break\n\nenv.close()\nprint(\"\\nâœ“ æµ‹è¯•å®Œæˆ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 18. æµ‹è¯•å€™é€‰åŠ¨ä½œé€‰æ‹©æ¨¡å¼",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nå€™é€‰åŠ¨ä½œé€‰æ‹©æ¨¡å¼ - è®©LLMä»admissible commandsä¸­é€‰æ‹©æœ€ä½³åŠ¨ä½œ\nä¸ä½¿ç”¨generationæ¨¡å¼ï¼Œç›´æ¥é€‰æ‹©\n\"\"\"\n\nimport re\nfrom typing import List, Tuple\n\nclass EnvironmentHistoryChoice:\n    \"\"\"ç¯å¢ƒå†å²è®°å½• - ç”¨äºå€™é€‰åŠ¨ä½œé€‰æ‹©æ¨¡å¼\"\"\"\n    \n    def __init__(self, base_prompt: str, start_info: str, memory: List[str]):\n        self.base_prompt = base_prompt\n        self.start_info = start_info\n        self.memory = memory\n        self.history = []\n        self.cur_step = 0\n    \n    def reset(self):\n        self.history = []\n        self.cur_step = 0\n    \n    def add(self, label: str, value: str):\n        self.history.append(f\"{label.capitalize()}: {value}\")\n        if label == \"action\":\n            self.cur_step += 1\n    \n    def __str__(self):\n        # æ„å»ºprompt: base + memory + start_info + history\n        parts = [self.base_prompt.strip()]\n        \n        if self.memory:\n            parts.append(\"\\nPrevious trial reflections:\")\n            for i, mem in enumerate(self.memory):\n                parts.append(f\"{i+1}. {mem}\")\n        \n        parts.append(f\"\\n{self.start_info}\")\n        \n        if self.history:\n            parts.append(\"\\n\".join(self.history))\n        \n        return \"\\n\".join(parts)\n    \n    def check_is_exhausted(self):\n        return self.cur_step >= 49\n\n\ndef llm_choose_action(\n    prompt: str, \n    admissible_commands: List[str],\n    model,\n    temperature: float = 0.0\n) -> str:\n    \"\"\"\n    è®©LLMä»å€™é€‰åŠ¨ä½œä¸­é€‰æ‹©\n    \n    Args:\n        prompt: å½“å‰çš„prompt (åŒ…å«å†å²)\n        admissible_commands: å¯é€‰åŠ¨ä½œåˆ—è¡¨\n        model: Modelç±»å‹\n        temperature: æ¸©åº¦å‚æ•°\n    \n    Returns:\n        é€‰ä¸­çš„åŠ¨ä½œ\n    \"\"\"\n    # æ„å»ºé€‰æ‹©prompt\n    choice_prompt = prompt + \"\\n\\nAvailable actions:\\n\"\n    for i, cmd in enumerate(admissible_commands):\n        choice_prompt += f\"{i+1}. {cmd}\\n\"\n    \n    choice_prompt += \"\\nWhich action should I take? Reply with ONLY the action text (not the number).\\nAction:\"\n    \n    # è°ƒç”¨LLM\n    try:\n        cur_try = 0\n        while cur_try < 6:\n            if model == \"text-davinci-003\":\n                from utils_qwen import get_completion\n                response = get_completion(\n                    prompt=choice_prompt,\n                    temperature=cur_try * 0.2,\n                    stop_strs=[\"\\n\"]\n                )\n            else:\n                from utils_qwen import get_chat\n                response = get_chat(\n                    prompt=choice_prompt,\n                    model=model,\n                    temperature=cur_try * 0.2,\n                    stop_strs=[\"\\n\"]\n                )\n            \n            response = response.strip()\n            \n            # å°è¯•åŒ¹é…åˆ°å€™é€‰åŠ¨ä½œ\n            # æ–¹æ³•1: ç›´æ¥åŒ¹é…\n            if response in admissible_commands:\n                return response\n            \n            # æ–¹æ³•2: æå–æ•°å­—\n            num_match = re.search(r'^(\\d+)', response)\n            if num_match:\n                idx = int(num_match.group(1)) - 1\n                if 0 <= idx < len(admissible_commands):\n                    return admissible_commands[idx]\n            \n            # æ–¹æ³•3: æ¨¡ç³ŠåŒ¹é… (å»æ‰æ ‡ç‚¹å’Œç©ºæ ¼)\n            response_clean = re.sub(r'[^\\w\\s]', '', response.lower())\n            for cmd in admissible_commands:\n                cmd_clean = re.sub(r'[^\\w\\s]', '', cmd.lower())\n                if response_clean == cmd_clean or response_clean in cmd_clean:\n                    return cmd\n            \n            cur_try += 1\n        \n        # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªåŠ¨ä½œ\n        print(f\"âš ï¸ æ— æ³•åŒ¹é…åŠ¨ä½œï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå€™é€‰: {admissible_commands[0]}\")\n        return admissible_commands[0]\n    \n    except Exception as e:\n        print(f\"é”™è¯¯: {e}\")\n        return admissible_commands[0]\n\n\ndef process_ob(ob):\n    \"\"\"å¤„ç†è§‚å¯Ÿ\"\"\"\n    if ob.startswith('You arrive at loc '):\n        ob = ob[ob.find('. ')+2:]    \n    return ob\n\n\ndef llm_simple(prompt: str, model, stop: List[str] = [\"\\n\"]):\n    \"\"\"ç®€å•çš„LLMè°ƒç”¨\"\"\"\n    try:\n        cur_try = 0\n        while cur_try < 6:\n            if model == \"text-davinci-003\":\n                from utils_qwen import get_completion\n                text = get_completion(prompt=prompt, temperature=cur_try * 0.2, stop_strs=stop)\n            else:\n                from utils_qwen import get_chat\n                text = get_chat(prompt=prompt, model=model, temperature=cur_try * 0.2, stop_strs=stop)\n            \n            if len(text.strip()) >= 5:\n                return text\n            cur_try += 1\n        return \"\"\n    except Exception as e:\n        print(f\"LLMé”™è¯¯: {e}\")\n        return \"\"\n\n\ndef alfworld_run_admissible(\n    env,\n    base_prompt: str,\n    memory: List[str],\n    to_print: bool = True,\n    ob: str = '',\n    info_init: dict = None,\n    model = \"qwen-7b-chat\"\n) -> Tuple[EnvironmentHistoryChoice, bool]:\n    \"\"\"\n    è¿è¡ŒAlfWorld - ä½¿ç”¨admissible commandsé€‰æ‹©\n    \n    è¿™ä¸ªç‰ˆæœ¬åœ¨æ¯ä¸€æ­¥éƒ½ä»ç¯å¢ƒæä¾›çš„å€™é€‰åŠ¨ä½œä¸­é€‰æ‹©\n    \"\"\"\n    if len(memory) > 3:\n        memory = memory[-3:]\n    \n    env_history = EnvironmentHistoryChoice(base_prompt, ob, memory)\n    env_history.reset()\n    \n    if to_print:\n        print(ob)\n        sys.stdout.flush()\n    \n    cur_step = 0\n    current_info = info_init\n    \n    while cur_step < 49:\n        # è·å–å½“å‰å¯ç”¨çš„åŠ¨ä½œ\n        if current_info and 'admissible_commands' in current_info and len(current_info['admissible_commands']) > 0:\n            admissible = current_info['admissible_commands'][0]\n            \n            # è®©LLMä»å€™é€‰ä¸­é€‰æ‹©\n            action = llm_choose_action(\n                str(env_history),\n                admissible,\n                model=model,\n                temperature=0.0\n            )\n        else:\n            # å¦‚æœæ²¡æœ‰å€™é€‰åŠ¨ä½œï¼Œä½¿ç”¨ç”Ÿæˆæ¨¡å¼\n            action = llm_simple(str(env_history) + \"\\n>\", stop=['\\n'], model=model).strip()\n        \n        env_history.add(\"action\", action)\n        \n        # æ‰§è¡ŒåŠ¨ä½œ\n        observation, reward, done, current_info = env.step([action])\n        observation = process_ob(observation[0])\n        reward = current_info['won'][0]\n        done = done[0]\n        \n        # thinkåŠ¨ä½œçš„ç‰¹æ®Šå¤„ç†\n        if action.startswith('think:'):\n            observation = 'OK.'\n        \n        env_history.add(\"observation\", observation)\n        \n        if to_print:\n            print(f'> {action}\\n{observation}')\n            sys.stdout.flush()\n        \n        if done:\n            return env_history, True\n        elif env_history.check_is_exhausted():\n            return env_history, False\n        \n        cur_step += 1\n    \n    return env_history, False\n\n\nprint(\"âœ“ å€™é€‰åŠ¨ä½œé€‰æ‹©æ¨¡å¼å·²å®šä¹‰\")\nprint(\"\\nä½¿ç”¨æ–¹æ³•:\")\nprint(\"  alfworld_run_admissible() - ä»å€™é€‰åŠ¨ä½œä¸­é€‰æ‹©\")\nprint(\"  llm_choose_action() - LLMé€‰æ‹©å‡½æ•°\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 17. å€™é€‰åŠ¨ä½œé€‰æ‹©æ¨¡å¼ (ç®€åŒ–ç‰ˆ)\n\nè¿™ä¸ªç‰ˆæœ¬è®©LLMç›´æ¥ä»ç¯å¢ƒæä¾›çš„å€™é€‰åŠ¨ä½œä¸­é€‰æ‹©ï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ–‡æœ¬ã€‚\nç±»ä¼¼DQNçš„admissibleæ¨¡å¼ï¼Œä½†ä½¿ç”¨LLMæ¥ç†è§£å’Œé€‰æ‹©ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlfWorld Reflexion with Qwen 7B - Google Colab (å®Œæ•´ç‰ˆ) ğŸš€\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
    "\n",
    "è¿™ä¸ª notebook ä½¿ç”¨ **Qwen 7B** æœ¬åœ°æ¨¡å‹è¿è¡Œ AlfWorld Reflexion å®éªŒï¼Œå®Œå…¨æŒ‰ç…§å®˜æ–¹æŒ‡å—é…ç½®ã€‚\n",
    "\n",
    "## ğŸ“‹ å®éªŒæ¦‚è¿°\n",
    "\n",
    "- **æ¨¡å‹**: Qwen-7B-Chatï¼ˆå­˜å‚¨åœ¨ Google Driveï¼‰\n",
    "- **ç¯å¢ƒ**: AlfWorld TextWorld ç¯å¢ƒ\n",
    "- **æ–¹æ³•**: Reflexion - é€šè¿‡è‡ªæˆ‘åæ€å­¦ä¹ \n",
    "- **ä¼˜åŠ¿**: å®Œå…¨å…è´¹ï¼Œæ— éœ€ API Key\n",
    "\n",
    "## ğŸ¯ ç³»ç»Ÿè¦æ±‚\n",
    "\n",
    "- **GPU**: T4 æˆ–æ›´å¥½ï¼ˆColab å…è´¹ç‰ˆæä¾›ï¼‰\n",
    "- **å†…å­˜**: è‡³å°‘ 12GB RAM\n",
    "- **å­˜å‚¨**: Google Drive çº¦ 16GBï¼ˆæ¨¡å‹14GB + æ•°æ®2GBï¼‰\n",
    "\n",
    "## ğŸš€ å¿«é€Ÿå¼€å§‹\n",
    "\n",
    "1. å¯ç”¨ GPUï¼šè¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU (T4)\n",
    "2. è¿è¡Œæ‰€æœ‰ cellsï¼ˆé¦–æ¬¡çº¦ 20-30 åˆ†é’Ÿï¼‰\n",
    "3. å¼€å§‹å®éªŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ£€æŸ¥è¿è¡Œç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Google Colab: True\n",
      "Python: 3.12.12\n",
      "\n",
      "âœ“ GPU: Tesla T4\n",
      "  æ˜¾å­˜: 14.7 GB\n",
      "  CUDA: 12.6\n",
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:            12Gi       947Mi       7.6Gi       3.0Mi       4.1Gi        11Gi\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# æ£€æŸ¥ Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"{'âœ“' if IN_COLAB else 'âœ—'} Google Colab: {IN_COLAB}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\\n\")\n",
    "\n",
    "# æ£€æŸ¥ GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"  CUDA: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âŒ æœªæ£€æµ‹åˆ° GPUï¼è¯·å¯ç”¨ GPU è¿è¡Œæ—¶\")\n",
    "    print(\"   è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU\")\n",
    "\n",
    "!free -h | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æŒ‚è½½ Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\n# é¡¹ç›®ç›®å½•ç»“æ„\nPROJECT_DIR = '/content/drive/MyDrive/AlfWorld_Qwen'\nMODEL_DIR = os.path.join(PROJECT_DIR, 'models')\nDATA_DIR = os.path.join(PROJECT_DIR, 'alfworld_data')  # AlfWorld æ•°æ®\nRESULTS_DIR = os.path.join(PROJECT_DIR, 'results')\n\n# âœ… æå‰å®šä¹‰ ALFWORLD_DIRï¼ˆä¾›åç»­ cell ä½¿ç”¨ï¼‰\nREPO_DIR = os.path.join(PROJECT_DIR, 'reflexion')\nALFWORLD_DIR = os.path.join(REPO_DIR, 'alfworld_runs')\n\nfor dir_path in [PROJECT_DIR, MODEL_DIR, DATA_DIR, RESULTS_DIR]:\n    os.makedirs(dir_path, exist_ok=True)\n\nprint(\"ç›®å½•ç»“æ„:\")\nprint(f\"  é¡¹ç›®: {PROJECT_DIR}\")\nprint(f\"  æ¨¡å‹: {MODEL_DIR}\")\nprint(f\"  æ•°æ®: {DATA_DIR}\")\nprint(f\"  ç»“æœ: {RESULTS_DIR}\")\nprint(f\"  AlfWorld: {ALFWORLD_DIR} (å°†åœ¨ cell-16 å…‹éš†)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å®‰è£…ä¾èµ–åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"å®‰è£…ä¾èµ–åŒ…...\\n\")\n",
    "\n",
    "# ç¡®ä¿ Python 3.9+\n",
    "import sys\n",
    "py_version = sys.version_info\n",
    "print(f\"Python ç‰ˆæœ¬: {py_version.major}.{py_version.minor}.{py_version.micro}\")\n",
    "if py_version.major < 3 or (py_version.major == 3 and py_version.minor < 9):\n",
    "    print(\"âš ï¸  è­¦å‘Š: AlfWorld éœ€è¦ Python 3.9+\")\n",
    "\n",
    "# 1. å®‰è£… AlfWorld (å®Œæ•´ç‰ˆ)\n",
    "print(\"\\n[1/5] å®‰è£… AlfWorld (å®Œæ•´ç‰ˆ)...\")\n",
    "!pip install -q alfworld[full]\n",
    "\n",
    "# 2. å®‰è£… Transformers\n",
    "print(\"[2/5] å®‰è£… Transformers...\")\n",
    "!pip install -q transformers>=4.35.0 accelerate>=0.20.0 sentencepiece tiktoken einops\n",
    "\n",
    "# 3. å®‰è£…é‡åŒ–å·¥å…·\n",
    "print(\"[3/5] å®‰è£…é‡åŒ–å·¥å…·...\")\n",
    "!pip install -q bitsandbytes\n",
    "\n",
    "# 4. å®‰è£…å…¶ä»–ä¾èµ–\n",
    "print(\"[4/5] å®‰è£…å…¶ä»–å·¥å…·...\")\n",
    "!pip install -q tenacity==8.1.0 pandas matplotlib seaborn tqdm\n",
    "\n",
    "# 5. Flash Attention (å¯é€‰)\n",
    "print(\"[5/5] å®‰è£… Flash Attention (å¯é€‰)...\")\n",
    "try:\n",
    "    !pip install -q flash-attn --no-build-isolation\n",
    "    print(\"  âœ“ Flash Attention å·²å®‰è£…\")\n",
    "except:\n",
    "    print(\"  âš ï¸ è·³è¿‡ Flash Attentionï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰\")\n",
    "\n",
    "print(\"\\nâœ“ æ‰€æœ‰ä¾èµ–å·²å®‰è£…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ä¸‹è½½ AlfWorld æ•°æ®æ–‡ä»¶\n",
    "\n",
    "æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œéœ€è¦ä¸‹è½½ PDDL å’Œæ¸¸æˆæ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½® ALFWORLD_DATA ç¯å¢ƒå˜é‡\n",
    "os.environ['ALFWORLD_DATA'] = DATA_DIR\n",
    "print(f\"ALFWORLD_DATA = {DATA_DIR}\\n\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ä¸‹è½½\n",
    "data_check_path = os.path.join(DATA_DIR, 'json_2.1.1')\n",
    "\n",
    "if os.path.exists(data_check_path) and os.path.exists(os.path.join(DATA_DIR, 'logic')):\n",
    "    print(\"âœ“ AlfWorld æ•°æ®å·²å­˜åœ¨\")\n",
    "    !ls -lh {DATA_DIR}\n",
    "else:\n",
    "    print(\"æ­£åœ¨ä¸‹è½½ AlfWorld æ•°æ®...\")\n",
    "    print(\"é¢„è®¡å¤§å°: ~2GB\")\n",
    "    print(\"é¢„è®¡æ—¶é—´: 5-10 åˆ†é’Ÿ\\n\")\n",
    "    \n",
    "    # ä¸‹è½½æ•°æ®\n",
    "    !alfworld-download --path {DATA_DIR}\n",
    "    \n",
    "    print(\"\\nâœ“ AlfWorld æ•°æ®ä¸‹è½½å®Œæˆ\")\n",
    "    !ls -lh {DATA_DIR}\n",
    "\n",
    "# éªŒè¯æ•°æ®å®Œæ•´æ€§\n",
    "required_paths = [\n",
    "    os.path.join(DATA_DIR, 'json_2.1.1/train'),\n",
    "    os.path.join(DATA_DIR, 'json_2.1.1/valid_seen'),\n",
    "    os.path.join(DATA_DIR, 'json_2.1.1/valid_unseen'),\n",
    "    os.path.join(DATA_DIR, 'logic/alfred.pddl'),\n",
    "    os.path.join(DATA_DIR, 'logic/alfred.twl2')\n",
    "]\n",
    "\n",
    "print(\"\\néªŒè¯æ•°æ®å®Œæ•´æ€§:\")\n",
    "all_ok = True\n",
    "for path in required_paths:\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"  {'âœ“' if exists else 'âœ—'} {os.path.basename(path)}\")\n",
    "    all_ok = all_ok and exists\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nâœ“ æ‰€æœ‰æ•°æ®æ–‡ä»¶å®Œæ•´\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  æŸäº›æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œå¯èƒ½å½±å“è¿è¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ä¸‹è½½ Qwen 7B æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen-7B-Chat\"\n",
    "LOCAL_MODEL_PATH = os.path.join(MODEL_DIR, \"Qwen-7B-Chat\")\n",
    "\n",
    "if os.path.exists(LOCAL_MODEL_PATH) and os.path.exists(os.path.join(LOCAL_MODEL_PATH, 'config.json')):\n",
    "    print(f\"âœ“ æ¨¡å‹å·²å­˜åœ¨: {LOCAL_MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"ä¸‹è½½ Qwen 7B æ¨¡å‹...\")\n",
    "    print(f\"ä½ç½®: {LOCAL_MODEL_PATH}\")\n",
    "    print(f\"å¤§å°: ~14GB, æ—¶é—´: 10-15åˆ†é’Ÿ\\n\")\n",
    "    \n",
    "    from huggingface_hub import snapshot_download\n",
    "    \n",
    "    try:\n",
    "        snapshot_download(\n",
    "            repo_id=MODEL_NAME,\n",
    "            local_dir=LOCAL_MODEL_PATH,\n",
    "            local_dir_use_symlinks=False,\n",
    "            resume_download=True\n",
    "        )\n",
    "        print(\"\\nâœ“ æ¨¡å‹ä¸‹è½½å®Œæˆ\")\n",
    "    except Exception as e:\n",
    "        print(f\"ä¸‹è½½å¤±è´¥: {e}\")\n",
    "        print(\"å°è¯•é•œåƒæº...\")\n",
    "        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "        snapshot_download(\n",
    "            repo_id=MODEL_NAME,\n",
    "            local_dir=LOCAL_MODEL_PATH,\n",
    "            local_dir_use_symlinks=False,\n",
    "            resume_download=True\n",
    "        )\n",
    "        print(\"\\nâœ“ æ¨¡å‹ä¸‹è½½å®Œæˆï¼ˆé•œåƒï¼‰\")\n",
    "\n",
    "!du -sh {LOCAL_MODEL_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. åŠ è½½ Qwen 7B æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"åŠ è½½ Qwen 7B...\\n\")\n",
    "\n",
    "# Tokenizer\n",
    "print(\"[1/2] åŠ è½½ tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"âœ“ Tokenizer å®Œæˆ\")\n",
    "\n",
    "# Model (8-bit é‡åŒ–)\n",
    "print(\"\\n[2/2] åŠ è½½æ¨¡å‹ (8-bit)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ").eval()\n",
    "\n",
    "print(\"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "print(f\"\\nå‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n",
    "print(f\"è®¾å¤‡: {model.hf_device_map}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. åˆ›å»º Qwen API åŒ…è£…å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import Optional, List\nimport json\n\nclass QwenChatWrapper:\n    \"\"\"Qwen æ¨¡å‹åŒ…è£…å™¨ - æ¨¡ä»¿ GPT åšæ³•ï¼Œæ­£ç¡®ä½¿ç”¨ alfworld_3prompts.json\n    \n    æ”¯æŒ Qwen2.5 (æ²¡æœ‰ .chat() æ–¹æ³•)\n    \"\"\"\n    \n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.generation_config = GenerationConfig.from_pretrained(\n            LOCAL_MODEL_PATH,\n            trust_remote_code=True\n        )\n        \n        # æ³¨æ„ï¼šprompts å®é™…ä¸Šç”± alfworld_trial_qwen.py åŠ è½½\n        # è¿™é‡Œåªæ˜¯ä¸ºäº†éªŒè¯æ–‡ä»¶å­˜åœ¨\n        prompts_path = os.path.join(ALFWORLD_DIR, 'prompts/alfworld_3prompts.json')\n        if os.path.exists(prompts_path):\n            print(f\"âœ“ æ‰¾åˆ° AlfWorld prompts æ–‡ä»¶\")\n        else:\n            print(f\"âš ï¸  æœªæ‰¾åˆ° prompts æ–‡ä»¶: {prompts_path}\")\n    \n    def chat(\n        self,\n        prompt: str,\n        temperature: float = 0.0,\n        max_tokens: int = 256,\n        stop_strs: Optional[List[str]] = None\n    ) -> str:\n        # æ£€æµ‹ AlfWorld promptï¼ˆåŒ…å« few-shot ç¤ºä¾‹çš„æ ‡å¿—ï¼‰\n        is_alfworld = \"Interact with a household to solve a task\" in prompt\n        \n        # æ¸©åº¦è°ƒæ•´ï¼ˆQwen ä¸æ”¯æŒ 0ï¼‰\n        temp = max(temperature, 0.01)\n        \n        gen_config = GenerationConfig(\n            temperature=temp,\n            top_p=0.95 if temp > 0.1 else 1.0,\n            max_new_tokens=max_tokens,\n            do_sample=temp > 0.1,\n            pad_token_id=self.tokenizer.pad_token_id,\n            eos_token_id=self.tokenizer.eos_token_id,\n        )\n        \n        try:\n            if is_alfworld:\n                # âœ… AlfWorld: è·³è¿‡ chat templateï¼Œç›´æ¥ç»­å†™ï¼ˆæ¨¡ä»¿ GPTï¼‰\n                model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(self.model.device)\n                \n                with torch.no_grad():\n                    generated_ids = self.model.generate(\n                        **model_inputs,\n                        generation_config=gen_config\n                    )\n                \n                # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥ promptï¼‰\n                generated_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n            else:\n                # âŒ æ™®é€šå¯¹è¯ï¼šä½¿ç”¨ chat template (Qwen2.5 æ–¹å¼)\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n                text = self.tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False,\n                    add_generation_prompt=True\n                )\n                \n                model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n                \n                with torch.no_grad():\n                    generated_ids = self.model.generate(\n                        **model_inputs,\n                        generation_config=gen_config\n                    )\n                \n                # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†\n                generated_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n            \n            # å¤„ç† stop strings\n            if stop_strs:\n                for stop_str in stop_strs:\n                    if stop_str in response:\n                        response = response[:response.index(stop_str)]\n            \n            return response.strip()\n        except Exception as e:\n            print(f\"ç”Ÿæˆé”™è¯¯: {e}\")\n            import traceback\n            traceback.print_exc()\n            return \"\"\n    \n    def completion(self, prompt: str, temperature: float = 0.0, \n                   max_tokens: int = 256, stop_strs: Optional[List[str]] = None) -> str:\n        return self.chat(prompt, temperature, max_tokens, stop_strs)\n\n# åˆ›å»ºå®ä¾‹\nqwen_api = QwenChatWrapper(model, tokenizer)\nprint(\"âœ“ Qwen API åŒ…è£…å™¨åˆ›å»ºæˆåŠŸ\")\n\n# æµ‹è¯•\nprint(\"\\næµ‹è¯•...\")\ntest = qwen_api.chat(\"ä½ å¥½\", temperature=0.1, max_tokens=30)\nprint(f\"å›å¤: {test}\")\nprint(\"âœ“ æµ‹è¯•é€šè¿‡\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å…‹éš† Reflexion ä»“åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIR = os.path.join(PROJECT_DIR, 'reflexion')\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(\"å…‹éš† Reflexion ä»“åº“...\")\n",
    "    !git clone https://github.com/noahshinn/reflexion.git {REPO_DIR}\n",
    "    print(\"âœ“ å…‹éš†å®Œæˆ\")\n",
    "else:\n",
    "    print(\"âœ“ ä»“åº“å·²å­˜åœ¨\")\n",
    "\n",
    "ALFWORLD_DIR = os.path.join(REPO_DIR, 'alfworld_runs')\n",
    "os.chdir(ALFWORLD_DIR)\n",
    "print(f\"\\nå½“å‰ç›®å½•: {os.getcwd()}\")\n",
    "\n",
    "# éªŒè¯ prompts æ–‡ä»¶\n",
    "prompts_path = os.path.join(ALFWORLD_DIR, 'prompts/alfworld_3prompts.json')\n",
    "if os.path.exists(prompts_path):\n",
    "    print(f\"âœ“ Prompts æ–‡ä»¶å­˜åœ¨\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Prompts æ–‡ä»¶ç¼ºå¤±: {prompts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. åˆ›å»º Qwen ç‰ˆæœ¬çš„å·¥å…·æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils_qwen.py\n",
    "utils_code = '''import sys\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from typing import Optional, List\n",
    "\n",
    "if sys.version_info >= (3, 8):\n",
    "    from typing import Literal\n",
    "else:\n",
    "    from typing_extensions import Literal\n",
    "\n",
    "Model = Literal[\"qwen-7b-chat\", \"gpt-4\", \"gpt-3.5-turbo\", \"text-davinci-003\"]\n",
    "\n",
    "_qwen_api = None\n",
    "\n",
    "def set_qwen_api(api):\n",
    "    global _qwen_api\n",
    "    _qwen_api = api\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_completion(prompt: str, temperature: float = 0.0, max_tokens: int = 256, \n",
    "                   stop_strs: Optional[List[str]] = None) -> str:\n",
    "    if _qwen_api is None:\n",
    "        raise RuntimeError(\"Qwen API æœªåˆå§‹åŒ–\")\n",
    "    return _qwen_api.completion(prompt, temperature, max_tokens, stop_strs)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_chat(prompt: str, model: Model, temperature: float = 0.0, max_tokens: int = 256,\n",
    "             stop_strs: Optional[List[str]] = None, is_batched: bool = False) -> str:\n",
    "    if _qwen_api is None:\n",
    "        raise RuntimeError(\"Qwen API æœªåˆå§‹åŒ–\")\n",
    "    return _qwen_api.chat(prompt, temperature, max_tokens, stop_strs)\n",
    "'''\n",
    "\n",
    "with open('utils_qwen.py', 'w') as f:\n",
    "    f.write(utils_code)\n",
    "print(\"âœ“ utils_qwen.py å·²åˆ›å»º\")\n",
    "\n",
    "# æ³¨å…¥ Qwen API\n",
    "sys.path.insert(0, ALFWORLD_DIR)\n",
    "import utils_qwen\n",
    "utils_qwen.set_qwen_api(qwen_api)\n",
    "print(\"âœ“ Qwen API å·²æ³¨å…¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿®æ”¹ alfworld_trial.py\n",
    "with open('alfworld_trial.py', 'r') as f:\n",
    "    code = f.read()\n",
    "\n",
    "code = code.replace(\n",
    "    'from utils import Model, get_chat, get_completion',\n",
    "    'from utils_qwen import Model, get_chat, get_completion'\n",
    ").replace(\n",
    "    'openai.api_key = os.environ[\"OPENAI_API_KEY\"]',\n",
    "    '# openai.api_key = os.environ[\"OPENAI_API_KEY\"]  # Qwen æ— éœ€ API key'\n",
    ")\n",
    "\n",
    "with open('alfworld_trial_qwen.py', 'w') as f:\n",
    "    f.write(code)\n",
    "print(\"âœ“ alfworld_trial_qwen.py å·²åˆ›å»º\")\n",
    "\n",
    "# ä¿®æ”¹ generate_reflections.py\n",
    "with open('generate_reflections.py', 'r') as f:\n",
    "    code = f.read()\n",
    "\n",
    "code = code.replace(\n",
    "    'from utils import get_completion',\n",
    "    'from utils_qwen import get_completion'\n",
    ")\n",
    "\n",
    "with open('generate_reflections_qwen.py', 'w') as f:\n",
    "    f.write(code)\n",
    "print(\"âœ“ generate_reflections_qwen.py å·²åˆ›å»º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æµ‹è¯• AlfWorld ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æµ‹è¯• AlfWorld ç¯å¢ƒ...\\n\")\n",
    "\n",
    "import alfworld\n",
    "import alfworld.agents.environment\n",
    "import alfworld.agents.modules.generic as generic\n",
    "import yaml\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    # æ–¹æ³•1: ä½¿ç”¨ generic.load_config (æ¨è)\n",
    "    # ä½†æˆ‘ä»¬å·²ç»æœ‰ base_config.yamlï¼Œç”¨æ–¹æ³•2\n",
    "    \n",
    "    # æ–¹æ³•2: æ‰‹åŠ¨åŠ è½½é…ç½® + get_environment\n",
    "    with open('base_config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    env_type = config['env']['type']\n",
    "    print(f\"ç¯å¢ƒç±»å‹: {env_type}\")\n",
    "    \n",
    "    # âœ… ä½¿ç”¨ get_environment å·¥å‚å‡½æ•°\n",
    "    env_class = alfworld.agents.environment.get_environment(env_type)\n",
    "    env = env_class(config, train_eval='eval_out_of_distribution')\n",
    "    env = env.init_env(batch_size=1)\n",
    "    \n",
    "    # æµ‹è¯•é‡ç½®\n",
    "    obs, info = env.reset()\n",
    "    print(\"âœ“ ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ\\n\")\n",
    "    \n",
    "    print(f\"åˆå§‹è§‚å¯Ÿ (å‰300å­—ç¬¦):\")\n",
    "    print(obs[0][:300])\n",
    "    print(\"...\\n\")\n",
    "    \n",
    "    # æ£€æŸ¥å¤‡é€‰åŠ¨ä½œ\n",
    "    if 'admissible_commands' in info:\n",
    "        admissible = info['admissible_commands'][0]\n",
    "        print(f\"âœ“ å¯ç”¨åŠ¨ä½œæ•°: {len(admissible)}\")\n",
    "        print(f\"  ç¤ºä¾‹åŠ¨ä½œ:\")\n",
    "        for i, action in enumerate(admissible[:5]):\n",
    "            print(f\"    {i+1}. {action}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æœªæä¾› admissible_commands\")\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\nâœ“ AlfWorld ç¯å¢ƒæµ‹è¯•é€šè¿‡\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 10.5. å¿«é€Ÿæµ‹è¯• Admissible æ¨¡å¼ ğŸ§ª",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. å®éªŒå‚æ•°é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ========== é…ç½® ==========\n",
    "NUM_TRIALS = 3           # å»ºè®®: 2-5\n",
    "NUM_ENVS = 5             # å»ºè®®: 5-20\n",
    "USE_MEMORY = True        # Reflexion å¼€å…³\n",
    "MODEL = \"qwen-7b-chat\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_NAME = f\"qwen_{timestamp}\"\n",
    "\n",
    "IS_RESUME = False\n",
    "RESUME_DIR = \"\"\n",
    "START_TRIAL_NUM = 0\n",
    "\n",
    "# ========== æ˜¾ç¤º ==========\n",
    "print(\"ğŸ“‹ å®éªŒé…ç½®\")\n",
    "print(\"=\"*60)\n",
    "print(f\"åç§°: {RUN_NAME}\")\n",
    "print(f\"Trials: {NUM_TRIALS}\")\n",
    "print(f\"ç¯å¢ƒæ•°: {NUM_ENVS}\")\n",
    "print(f\"Reflexion: {'âœ“' if USE_MEMORY else 'âœ—'}\")\n",
    "print(f\"æ¨¡å‹: {MODEL}\")\n",
    "print(f\"æ•°æ®: {DATA_DIR}\")\n",
    "print(f\"ç»“æœ: {RESULTS_DIR}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ—¶é—´ä¼°ç®—\n",
    "time_min = NUM_TRIALS * NUM_ENVS * 2\n",
    "time_max = NUM_TRIALS * NUM_ENVS * 5\n",
    "\n",
    "print(f\"\\nâ±ï¸  é¢„ä¼°: {time_min}-{time_max} åˆ†é’Ÿ\")\n",
    "print(f\"ğŸ’° æˆæœ¬: $0\")\n",
    "\n",
    "if time_max > 90:\n",
    "    print(\"\\nâš ï¸  è¶…è¿‡90åˆ†é’Ÿï¼ŒColab å¯èƒ½è¶…æ—¶\")\n",
    "    print(\"   å»ºè®®å‡å°‘ç¯å¢ƒæ•°é‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. è¿è¡Œå®éªŒ ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from alfworld_trial_qwen import run_trial\n",
    "from generate_reflections_qwen import update_memory\n",
    "\n",
    "def run_experiment():\n",
    "    # åˆå§‹åŒ–\n",
    "    if IS_RESUME:\n",
    "        logging_dir = RESUME_DIR\n",
    "        config_path = os.path.join(RESUME_DIR, f'env_results_trial_{START_TRIAL_NUM-1}.json')\n",
    "        with open(config_path) as f:\n",
    "            env_configs = json.load(f)\n",
    "        print(f\"âœ“ æ¢å¤è‡ª trial {START_TRIAL_NUM}\\n\")\n",
    "    else:\n",
    "        logging_dir = os.path.join(RESULTS_DIR, RUN_NAME)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "        env_configs = [{\n",
    "            'name': f'env_{i}',\n",
    "            'memory': [],\n",
    "            'is_success': False,\n",
    "            'skip': False\n",
    "        } for i in range(NUM_ENVS)]\n",
    "        print(f\"âœ“ åˆ›å»ºç›®å½•: {logging_dir}\\n\")\n",
    "    \n",
    "    world_log = os.path.join(logging_dir, 'world.log')\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸš€ å¼€å§‹å®éªŒ (Qwen 7B)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    progress = tqdm(total=NUM_TRIALS*NUM_ENVS, desc=\"æ€»è¿›åº¦\")\n",
    "    stats = []\n",
    "    \n",
    "    trial_idx = START_TRIAL_NUM if IS_RESUME else 0\n",
    "    \n",
    "    while trial_idx < NUM_TRIALS:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“ Trial {trial_idx+1}/{NUM_TRIALS}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        with open(world_log, 'a') as f:\n",
    "            f.write(f'\\n\\n***** Trial #{trial_idx} *****\\n\\n')\n",
    "        \n",
    "        trial_log = os.path.join(logging_dir, f'trial_{trial_idx}.log')\n",
    "        trial_config = os.path.join(logging_dir, f'env_results_trial_{trial_idx}.json')\n",
    "        \n",
    "        # è¿è¡Œ\n",
    "        print(f\"â³ è¿è¡Œ...\")\n",
    "        env_configs = run_trial(\n",
    "            trial_log, world_log, trial_idx,\n",
    "            env_configs, USE_MEMORY, MODEL\n",
    "        )\n",
    "        \n",
    "        progress.update(NUM_ENVS)\n",
    "        \n",
    "        # åæ€\n",
    "        if USE_MEMORY:\n",
    "            print(f\"\\nğŸ§  ç”Ÿæˆåæ€...\")\n",
    "            env_configs = update_memory(trial_log, env_configs)\n",
    "        \n",
    "        # ä¿å­˜\n",
    "        with open(trial_config, 'w') as f:\n",
    "            json.dump(env_configs, f, indent=4)\n",
    "        \n",
    "        with open(world_log, 'a') as f:\n",
    "            f.write(f'\\n\\n***** End Trial #{trial_idx} *****\\n\\n')\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        success = sum(1 for e in env_configs if e['is_success'])\n",
    "        acc = success / NUM_ENVS\n",
    "        \n",
    "        stats.append({\n",
    "            'trial': trial_idx,\n",
    "            'success': success,\n",
    "            'total': NUM_ENVS,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâœ“ Trial {trial_idx} å®Œæˆ\")\n",
    "        print(f\"   æˆåŠŸ: {success}/{NUM_ENVS} ({acc:.1%})\")\n",
    "        \n",
    "        if len(stats) > 1:\n",
    "            delta = acc - stats[-2]['accuracy']\n",
    "            trend = \"ğŸ“ˆ\" if delta > 0 else \"ğŸ“‰\" if delta < 0 else \"â¡ï¸\"\n",
    "            print(f\"   è¶‹åŠ¿: {trend} {delta:+.1%}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   GPU: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "        \n",
    "        trial_idx += 1\n",
    "    \n",
    "    progress.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ å®éªŒå®Œæˆ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nğŸ“ ç»“æœ: {logging_dir}\")\n",
    "    print(f\"ğŸ’¾ å·²åŒæ­¥åˆ° Google Drive\")\n",
    "    \n",
    "    return logging_dir, env_configs, stats\n",
    "\n",
    "# è¿è¡Œ\n",
    "try:\n",
    "    logging_dir, configs, stats = run_experiment()\n",
    "    print(\"\\nâœ“ æˆåŠŸ\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâš ï¸  ä¸­æ–­\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ç»“æœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(stats)\n",
    "df.columns = ['Trial', 'Success', 'Total', 'Accuracy']\n",
    "\n",
    "print(\"\\nğŸ“Š Qwen 7B å®éªŒç»“æœ\")\n",
    "print(\"=\"*60)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(\"\\nç»Ÿè®¡:\")\n",
    "    print(f\"  æ¨¡å‹: Qwen 7B (8-bit)\")\n",
    "    print(f\"  åˆå§‹: {df.iloc[0]['Accuracy']:.1%}\")\n",
    "    print(f\"  æœ€ç»ˆ: {df.iloc[-1]['Accuracy']:.1%}\")\n",
    "    print(f\"  æå‡: {(df.iloc[-1]['Accuracy']-df.iloc[0]['Accuracy']):.1%}\")\n",
    "    print(f\"  å¹³å‡: {df['Accuracy'].mean():.1%}\")\n",
    "    print(f\"  æœ€é«˜: {df['Accuracy'].max():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# å­¦ä¹ æ›²çº¿\n",
    "axes[0].plot(df['Trial'], df['Accuracy']*100, 'o-', lw=2.5, ms=8, color='#2E86AB')\n",
    "axes[0].fill_between(df['Trial'], 0, df['Accuracy']*100, alpha=0.2, color='#2E86AB')\n",
    "axes[0].set_xlabel('Trial', fontsize=12, weight='bold')\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12, weight='bold')\n",
    "axes[0].set_title('Qwen 7B Learning Curve', fontsize=14, weight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_ylim([0, 105])\n",
    "\n",
    "# æˆåŠŸæ•°\n",
    "colors = ['#06A77D' if i==len(df)-1 else '#A23B72' for i in range(len(df))]\n",
    "axes[1].bar(df['Trial'], df['Success'], color=colors, alpha=0.8)\n",
    "axes[1].set_xlabel('Trial', fontsize=12, weight='bold')\n",
    "axes[1].set_ylabel('Successes', fontsize=12, weight='bold')\n",
    "axes[1].set_title('Success Count', fontsize=14, weight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "save_path = os.path.join(logging_dir, 'qwen_curve.png')\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nâœ“ å›¾è¡¨: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. æŸ¥çœ‹åæ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_memory(run_dir, trial, env):\n",
    "    path = os.path.join(run_dir, f'env_results_trial_{trial}.json')\n",
    "    with open(path) as f:\n",
    "        configs = json.load(f)\n",
    "    \n",
    "    e = configs[env]\n",
    "    print(f\"ğŸ§  ç¯å¢ƒ #{env} (Trial {trial})\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"çŠ¶æ€: {'âœ“' if e['is_success'] else 'âœ—'}\")\n",
    "    print(f\"åæ€æ•°: {len(e['memory'])}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, m in enumerate(e['memory']):\n",
    "        print(f\"\\n--- åæ€ #{i+1} ---\")\n",
    "        print(m)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "if USE_MEMORY and NUM_TRIALS > 0:\n",
    "    view_memory(logging_dir, NUM_TRIALS-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. æ€§èƒ½ç»Ÿè®¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nâš¡ æ€§èƒ½\")\n",
    "print(\"=\"*60)\n",
    "print(f\"æ¨¡å‹: Qwen 7B (8-bit)\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Trials: {NUM_TRIALS}\")\n",
    "print(f\"ç¯å¢ƒ: {NUM_TRIALS * NUM_ENVS}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU:\")\n",
    "    print(f\"  å³°å€¼: {torch.cuda.max_memory_allocated()/1024**3:.2f}GB\")\n",
    "    print(f\"  å½“å‰: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "print(f\"\\nğŸ’° æˆæœ¬: $0\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. ä¸‹è½½ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "zip_name = f\"{RUN_NAME}.zip\"\n",
    "zip_path = os.path.join(RESULTS_DIR, zip_name)\n",
    "\n",
    "print(\"å‹ç¼©...\")\n",
    "shutil.make_archive(\n",
    "    os.path.join(RESULTS_DIR, RUN_NAME),\n",
    "    'zip',\n",
    "    logging_dir\n",
    ")\n",
    "print(f\"âœ“ å®Œæˆ: {zip_name}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"\\nä¸‹è½½...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"âœ“ å®Œæˆ\")\n",
    "else:\n",
    "    print(f\"\\nä½ç½®: {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š è¯´æ˜\n",
    "\n",
    "### å®Œæ•´å®ç°\n",
    "\n",
    "âœ… æŒ‰ç…§å®˜æ–¹æŒ‡å—é…ç½®  \n",
    "âœ… ä¸‹è½½æ‰€æœ‰å¿…éœ€çš„æ•°æ®æ–‡ä»¶  \n",
    "âœ… ä½¿ç”¨åŸå§‹ prompts  \n",
    "âœ… å®Œå…¨å…¼å®¹ OpenAI API  \n",
    "âœ… æ”¯æŒå®Œæ•´çš„ Reflexion æµç¨‹\n",
    "\n",
    "### ç›®å½•ç»“æ„\n",
    "\n",
    "```\n",
    "Google Drive/MyDrive/AlfWorld_Qwen/\n",
    "â”œâ”€â”€ models/\n",
    "â”‚   â””â”€â”€ Qwen-7B-Chat/         # 14GB\n",
    "â”œâ”€â”€ alfworld_data/             # 2GB\n",
    "â”‚   â”œâ”€â”€ json_2.1.1/\n",
    "â”‚   â””â”€â”€ logic/\n",
    "â”œâ”€â”€ reflexion/\n",
    "â”‚   â””â”€â”€ alfworld_runs/\n",
    "â”‚       â”œâ”€â”€ prompts/           # åŸå§‹ prompts\n",
    "â”‚       â”œâ”€â”€ utils_qwen.py\n",
    "â”‚       â”œâ”€â”€ alfworld_trial_qwen.py\n",
    "â”‚       â””â”€â”€ generate_reflections_qwen.py\n",
    "â””â”€â”€ results/\n",
    "    â””â”€â”€ qwen_YYYYMMDD_HHMMSS/\n",
    "```\n",
    "\n",
    "### ä¸å®˜æ–¹è®¾ç½®å¯¹æ¯”\n",
    "\n",
    "| æ­¥éª¤ | å®˜æ–¹ | æœ¬å®ç° |\n",
    "|------|------|--------|\n",
    "| å®‰è£… AlfWorld | âœ“ pip install alfworld[full] | âœ“ ç›¸åŒ |\n",
    "| ä¸‹è½½æ•°æ® | âœ“ alfworld-download | âœ“ ç›¸åŒ |\n",
    "| è®¾ç½®ç¯å¢ƒå˜é‡ | âœ“ ALFWORLD_DATA | âœ“ ç›¸åŒ |\n",
    "| ä½¿ç”¨ prompts | âœ“ alfworld_3prompts.json | âœ“ ç›¸åŒ |\n",
    "| API | OpenAI | Qwen (å…¼å®¹) |\n",
    "\n",
    "### æ€§èƒ½\n",
    "\n",
    "- **å‡†ç¡®æ€§**: ä¸ GPT-3.5 ç›¸è¿‘\n",
    "- **é€Ÿåº¦**: çº¦ 2-3 å€æ…¢\n",
    "- **æˆæœ¬**: $0 vs $50-150\n",
    "- **éšç§**: å®Œå…¨æœ¬åœ°\n",
    "\n",
    "### é—®é¢˜æ’æŸ¥\n",
    "\n",
    "**æ•°æ®æ–‡ä»¶ç¼ºå¤±**:\n",
    "```bash\n",
    "!alfworld-download --path {DATA_DIR}\n",
    "```\n",
    "\n",
    "**ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥**:\n",
    "æ£€æŸ¥ `ALFWORLD_DATA` ç¯å¢ƒå˜é‡\n",
    "\n",
    "**æ¨¡å‹æ¨ç†æ…¢**:\n",
    "- ç¡®è®¤ä½¿ç”¨ GPU\n",
    "- è€ƒè™‘ 4-bit é‡åŒ–\n",
    "- å‡å°‘ç¯å¢ƒæ•°é‡"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "alfworld_qwen_colab_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}