{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlfWorld Reflexion with Qwen 7B - Google Colab (å®Œæ•´ç‰ˆ) ğŸš€\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
    "\n",
    "è¿™ä¸ª notebook ä½¿ç”¨ **Qwen 7B** æœ¬åœ°æ¨¡å‹è¿è¡Œ AlfWorld Reflexion å®éªŒï¼Œå®Œå…¨æŒ‰ç…§å®˜æ–¹æŒ‡å—é…ç½®ã€‚\n",
    "\n",
    "## ğŸ“‹ å®éªŒæ¦‚è¿°\n",
    "\n",
    "- **æ¨¡å‹**: Qwen-7B-Chatï¼ˆå­˜å‚¨åœ¨ Google Driveï¼‰\n",
    "- **ç¯å¢ƒ**: AlfWorld TextWorld ç¯å¢ƒ\n",
    "- **æ–¹æ³•**: Reflexion - é€šè¿‡è‡ªæˆ‘åæ€å­¦ä¹ \n",
    "- **ä¼˜åŠ¿**: å®Œå…¨å…è´¹ï¼Œæ— éœ€ API Key\n",
    "\n",
    "## ğŸ¯ ç³»ç»Ÿè¦æ±‚\n",
    "\n",
    "- **GPU**: T4 æˆ–æ›´å¥½ï¼ˆColab å…è´¹ç‰ˆæä¾›ï¼‰\n",
    "- **å†…å­˜**: è‡³å°‘ 12GB RAM\n",
    "- **å­˜å‚¨**: Google Drive çº¦ 16GBï¼ˆæ¨¡å‹14GB + æ•°æ®2GBï¼‰\n",
    "\n",
    "## ğŸš€ å¿«é€Ÿå¼€å§‹\n",
    "\n",
    "1. å¯ç”¨ GPUï¼šè¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU (T4)\n",
    "2. è¿è¡Œæ‰€æœ‰ cellsï¼ˆé¦–æ¬¡çº¦ 20-30 åˆ†é’Ÿï¼‰\n",
    "3. å¼€å§‹å®éªŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ£€æŸ¥è¿è¡Œç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Google Colab: True\n",
      "Python: 3.12.12\n",
      "\n",
      "âœ“ GPU: Tesla T4\n",
      "  æ˜¾å­˜: 14.7 GB\n",
      "  CUDA: 12.6\n",
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:            12Gi       947Mi       7.6Gi       3.0Mi       4.1Gi        11Gi\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# æ£€æŸ¥ Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"{'âœ“' if IN_COLAB else 'âœ—'} Google Colab: {IN_COLAB}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\\n\")\n",
    "\n",
    "# æ£€æŸ¥ GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"  CUDA: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âŒ æœªæ£€æµ‹åˆ° GPUï¼è¯·å¯ç”¨ GPU è¿è¡Œæ—¶\")\n",
    "    print(\"   è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU\")\n",
    "\n",
    "!free -h | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æŒ‚è½½ Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1150240544.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# é¡¹ç›®ç›®å½•ç»“æ„\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPROJECT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/AlfWorld_Qwen'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# é¡¹ç›®ç›®å½•ç»“æ„\n",
    "PROJECT_DIR = '/content/drive/MyDrive/AlfWorld_Qwen'\n",
    "MODEL_DIR = os.path.join(PROJECT_DIR, 'models')\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'alfworld_data')  # AlfWorld æ•°æ®\n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR, 'results')\n",
    "\n",
    "for dir_path in [PROJECT_DIR, MODEL_DIR, DATA_DIR, RESULTS_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"ç›®å½•ç»“æ„:\")\n",
    "print(f\"  é¡¹ç›®: {PROJECT_DIR}\")\n",
    "print(f\"  æ¨¡å‹: {MODEL_DIR}\")\n",
    "print(f\"  æ•°æ®: {DATA_DIR}\")\n",
    "print(f\"  ç»“æœ: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å®‰è£…ä¾èµ–åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"å®‰è£…ä¾èµ–åŒ…...\\n\")\n",
    "\n",
    "# ç¡®ä¿ Python 3.9+\n",
    "import sys\n",
    "py_version = sys.version_info\n",
    "print(f\"Python ç‰ˆæœ¬: {py_version.major}.{py_version.minor}.{py_version.micro}\")\n",
    "if py_version.major < 3 or (py_version.major == 3 and py_version.minor < 9):\n",
    "    print(\"âš ï¸  è­¦å‘Š: AlfWorld éœ€è¦ Python 3.9+\")\n",
    "\n",
    "# 1. å®‰è£… AlfWorld (å®Œæ•´ç‰ˆ)\n",
    "print(\"\\n[1/5] å®‰è£… AlfWorld (å®Œæ•´ç‰ˆ)...\")\n",
    "!pip install -q alfworld[full]\n",
    "\n",
    "# 2. å®‰è£… Transformers\n",
    "print(\"[2/5] å®‰è£… Transformers...\")\n",
    "!pip install -q transformers>=4.35.0 accelerate>=0.20.0 sentencepiece tiktoken einops\n",
    "\n",
    "# 3. å®‰è£…é‡åŒ–å·¥å…·\n",
    "print(\"[3/5] å®‰è£…é‡åŒ–å·¥å…·...\")\n",
    "!pip install -q bitsandbytes\n",
    "\n",
    "# 4. å®‰è£…å…¶ä»–ä¾èµ–\n",
    "print(\"[4/5] å®‰è£…å…¶ä»–å·¥å…·...\")\n",
    "!pip install -q tenacity==8.1.0 pandas matplotlib seaborn tqdm\n",
    "\n",
    "# 5. Flash Attention (å¯é€‰)\n",
    "print(\"[5/5] å®‰è£… Flash Attention (å¯é€‰)...\")\n",
    "try:\n",
    "    !pip install -q flash-attn --no-build-isolation\n",
    "    print(\"  âœ“ Flash Attention å·²å®‰è£…\")\n",
    "except:\n",
    "    print(\"  âš ï¸ è·³è¿‡ Flash Attentionï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰\")\n",
    "\n",
    "print(\"\\nâœ“ æ‰€æœ‰ä¾èµ–å·²å®‰è£…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ä¸‹è½½ AlfWorld æ•°æ®æ–‡ä»¶\n",
    "\n",
    "æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œéœ€è¦ä¸‹è½½ PDDL å’Œæ¸¸æˆæ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½® ALFWORLD_DATA ç¯å¢ƒå˜é‡\n",
    "os.environ['ALFWORLD_DATA'] = DATA_DIR\n",
    "print(f\"ALFWORLD_DATA = {DATA_DIR}\\n\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ä¸‹è½½\n",
    "data_check_path = os.path.join(DATA_DIR, 'json_2.1.1')\n",
    "\n",
    "if os.path.exists(data_check_path) and os.path.exists(os.path.join(DATA_DIR, 'logic')):\n",
    "    print(\"âœ“ AlfWorld æ•°æ®å·²å­˜åœ¨\")\n",
    "    !ls -lh {DATA_DIR}\n",
    "else:\n",
    "    print(\"æ­£åœ¨ä¸‹è½½ AlfWorld æ•°æ®...\")\n",
    "    print(\"é¢„è®¡å¤§å°: ~2GB\")\n",
    "    print(\"é¢„è®¡æ—¶é—´: 5-10 åˆ†é’Ÿ\\n\")\n",
    "    \n",
    "    # ä¸‹è½½æ•°æ®\n",
    "    !alfworld-download --path {DATA_DIR}\n",
    "    \n",
    "    print(\"\\nâœ“ AlfWorld æ•°æ®ä¸‹è½½å®Œæˆ\")\n",
    "    !ls -lh {DATA_DIR}\n",
    "\n",
    "# éªŒè¯æ•°æ®å®Œæ•´æ€§\n",
    "required_paths = [\n",
    "    os.path.join(DATA_DIR, 'json_2.1.1/train'),\n",
    "    os.path.join(DATA_DIR, 'json_2.1.1/valid_seen'),\n",
    "    os.path.join(DATA_DIR, 'json_2.1.1/valid_unseen'),\n",
    "    os.path.join(DATA_DIR, 'logic/alfred.pddl'),\n",
    "    os.path.join(DATA_DIR, 'logic/alfred.twl2')\n",
    "]\n",
    "\n",
    "print(\"\\néªŒè¯æ•°æ®å®Œæ•´æ€§:\")\n",
    "all_ok = True\n",
    "for path in required_paths:\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"  {'âœ“' if exists else 'âœ—'} {os.path.basename(path)}\")\n",
    "    all_ok = all_ok and exists\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nâœ“ æ‰€æœ‰æ•°æ®æ–‡ä»¶å®Œæ•´\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  æŸäº›æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œå¯èƒ½å½±å“è¿è¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ä¸‹è½½ Qwen 7B æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen-7B-Chat\"\n",
    "LOCAL_MODEL_PATH = os.path.join(MODEL_DIR, \"Qwen-7B-Chat\")\n",
    "\n",
    "if os.path.exists(LOCAL_MODEL_PATH) and os.path.exists(os.path.join(LOCAL_MODEL_PATH, 'config.json')):\n",
    "    print(f\"âœ“ æ¨¡å‹å·²å­˜åœ¨: {LOCAL_MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"ä¸‹è½½ Qwen 7B æ¨¡å‹...\")\n",
    "    print(f\"ä½ç½®: {LOCAL_MODEL_PATH}\")\n",
    "    print(f\"å¤§å°: ~14GB, æ—¶é—´: 10-15åˆ†é’Ÿ\\n\")\n",
    "    \n",
    "    from huggingface_hub import snapshot_download\n",
    "    \n",
    "    try:\n",
    "        snapshot_download(\n",
    "            repo_id=MODEL_NAME,\n",
    "            local_dir=LOCAL_MODEL_PATH,\n",
    "            local_dir_use_symlinks=False,\n",
    "            resume_download=True\n",
    "        )\n",
    "        print(\"\\nâœ“ æ¨¡å‹ä¸‹è½½å®Œæˆ\")\n",
    "    except Exception as e:\n",
    "        print(f\"ä¸‹è½½å¤±è´¥: {e}\")\n",
    "        print(\"å°è¯•é•œåƒæº...\")\n",
    "        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "        snapshot_download(\n",
    "            repo_id=MODEL_NAME,\n",
    "            local_dir=LOCAL_MODEL_PATH,\n",
    "            local_dir_use_symlinks=False,\n",
    "            resume_download=True\n",
    "        )\n",
    "        print(\"\\nâœ“ æ¨¡å‹ä¸‹è½½å®Œæˆï¼ˆé•œåƒï¼‰\")\n",
    "\n",
    "!du -sh {LOCAL_MODEL_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. åŠ è½½ Qwen 7B æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"åŠ è½½ Qwen 7B...\\n\")\n",
    "\n",
    "# Tokenizer\n",
    "print(\"[1/2] åŠ è½½ tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"âœ“ Tokenizer å®Œæˆ\")\n",
    "\n",
    "# Model (8-bit é‡åŒ–)\n",
    "print(\"\\n[2/2] åŠ è½½æ¨¡å‹ (8-bit)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ").eval()\n",
    "\n",
    "print(\"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "print(f\"\\nå‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n",
    "print(f\"è®¾å¤‡: {model.hf_device_map}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. åˆ›å»º Qwen API åŒ…è£…å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "class QwenChatWrapper:\n",
    "    \"\"\"Qwen æ¨¡å‹åŒ…è£…å™¨ - å…¼å®¹ OpenAI API\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.generation_config = GenerationConfig.from_pretrained(\n",
    "            LOCAL_MODEL_PATH,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        temperature: float = 0.0,\n",
    "        max_tokens: int = 256,\n",
    "        stop_strs: Optional[List[str]] = None\n",
    "    ) -> str:\n",
    "        # æ¸©åº¦è°ƒæ•´ï¼ˆQwen ä¸æ”¯æŒ 0ï¼‰\n",
    "        temp = max(temperature, 0.01)\n",
    "        \n",
    "        gen_config = GenerationConfig(\n",
    "            temperature=temp,\n",
    "            top_p=0.95 if temp > 0.1 else 1.0,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=temp > 0.1,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response, _ = self.model.chat(\n",
    "                self.tokenizer,\n",
    "                prompt,\n",
    "                history=None,\n",
    "                generation_config=gen_config\n",
    "            )\n",
    "            \n",
    "            # å¤„ç† stop strings\n",
    "            if stop_strs:\n",
    "                for stop_str in stop_strs:\n",
    "                    if stop_str in response:\n",
    "                        response = response[:response.index(stop_str)]\n",
    "            \n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"ç”Ÿæˆé”™è¯¯: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def completion(self, prompt: str, temperature: float = 0.0, \n",
    "                   max_tokens: int = 256, stop_strs: Optional[List[str]] = None) -> str:\n",
    "        return self.chat(prompt, temperature, max_tokens, stop_strs)\n",
    "\n",
    "# åˆ›å»ºå®ä¾‹\n",
    "qwen_api = QwenChatWrapper(model, tokenizer)\n",
    "print(\"âœ“ Qwen API åŒ…è£…å™¨åˆ›å»ºæˆåŠŸ\")\n",
    "\n",
    "# æµ‹è¯•\n",
    "print(\"\\næµ‹è¯•...\")\n",
    "test = qwen_api.chat(\"ä½ å¥½\", temperature=0.1, max_tokens=30)\n",
    "print(f\"å›å¤: {test}\")\n",
    "print(\"âœ“ æµ‹è¯•é€šè¿‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å…‹éš† Reflexion ä»“åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIR = os.path.join(PROJECT_DIR, 'reflexion')\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(\"å…‹éš† Reflexion ä»“åº“...\")\n",
    "    !git clone https://github.com/noahshinn/reflexion.git {REPO_DIR}\n",
    "    print(\"âœ“ å…‹éš†å®Œæˆ\")\n",
    "else:\n",
    "    print(\"âœ“ ä»“åº“å·²å­˜åœ¨\")\n",
    "\n",
    "ALFWORLD_DIR = os.path.join(REPO_DIR, 'alfworld_runs')\n",
    "os.chdir(ALFWORLD_DIR)\n",
    "print(f\"\\nå½“å‰ç›®å½•: {os.getcwd()}\")\n",
    "\n",
    "# éªŒè¯ prompts æ–‡ä»¶\n",
    "prompts_path = os.path.join(ALFWORLD_DIR, 'prompts/alfworld_3prompts.json')\n",
    "if os.path.exists(prompts_path):\n",
    "    print(f\"âœ“ Prompts æ–‡ä»¶å­˜åœ¨\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Prompts æ–‡ä»¶ç¼ºå¤±: {prompts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. åˆ›å»º Qwen ç‰ˆæœ¬çš„å·¥å…·æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils_qwen.py\n",
    "utils_code = '''import sys\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from typing import Optional, List\n",
    "\n",
    "if sys.version_info >= (3, 8):\n",
    "    from typing import Literal\n",
    "else:\n",
    "    from typing_extensions import Literal\n",
    "\n",
    "Model = Literal[\"qwen-7b-chat\", \"gpt-4\", \"gpt-3.5-turbo\", \"text-davinci-003\"]\n",
    "\n",
    "_qwen_api = None\n",
    "\n",
    "def set_qwen_api(api):\n",
    "    global _qwen_api\n",
    "    _qwen_api = api\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_completion(prompt: str, temperature: float = 0.0, max_tokens: int = 256, \n",
    "                   stop_strs: Optional[List[str]] = None) -> str:\n",
    "    if _qwen_api is None:\n",
    "        raise RuntimeError(\"Qwen API æœªåˆå§‹åŒ–\")\n",
    "    return _qwen_api.completion(prompt, temperature, max_tokens, stop_strs)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_chat(prompt: str, model: Model, temperature: float = 0.0, max_tokens: int = 256,\n",
    "             stop_strs: Optional[List[str]] = None, is_batched: bool = False) -> str:\n",
    "    if _qwen_api is None:\n",
    "        raise RuntimeError(\"Qwen API æœªåˆå§‹åŒ–\")\n",
    "    return _qwen_api.chat(prompt, temperature, max_tokens, stop_strs)\n",
    "'''\n",
    "\n",
    "with open('utils_qwen.py', 'w') as f:\n",
    "    f.write(utils_code)\n",
    "print(\"âœ“ utils_qwen.py å·²åˆ›å»º\")\n",
    "\n",
    "# æ³¨å…¥ Qwen API\n",
    "sys.path.insert(0, ALFWORLD_DIR)\n",
    "import utils_qwen\n",
    "utils_qwen.set_qwen_api(qwen_api)\n",
    "print(\"âœ“ Qwen API å·²æ³¨å…¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿®æ”¹ alfworld_trial.py\n",
    "with open('alfworld_trial.py', 'r') as f:\n",
    "    code = f.read()\n",
    "\n",
    "code = code.replace(\n",
    "    'from utils import Model, get_chat, get_completion',\n",
    "    'from utils_qwen import Model, get_chat, get_completion'\n",
    ").replace(\n",
    "    'openai.api_key = os.environ[\"OPENAI_API_KEY\"]',\n",
    "    '# openai.api_key = os.environ[\"OPENAI_API_KEY\"]  # Qwen æ— éœ€ API key'\n",
    ")\n",
    "\n",
    "with open('alfworld_trial_qwen.py', 'w') as f:\n",
    "    f.write(code)\n",
    "print(\"âœ“ alfworld_trial_qwen.py å·²åˆ›å»º\")\n",
    "\n",
    "# ä¿®æ”¹ generate_reflections.py\n",
    "with open('generate_reflections.py', 'r') as f:\n",
    "    code = f.read()\n",
    "\n",
    "code = code.replace(\n",
    "    'from utils import get_completion',\n",
    "    'from utils_qwen import get_completion'\n",
    ")\n",
    "\n",
    "with open('generate_reflections_qwen.py', 'w') as f:\n",
    "    f.write(code)\n",
    "print(\"âœ“ generate_reflections_qwen.py å·²åˆ›å»º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æµ‹è¯• AlfWorld ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æµ‹è¯• AlfWorld ç¯å¢ƒ...\\n\")\n",
    "\n",
    "import alfworld\n",
    "import alfworld.agents.environment\n",
    "import alfworld.agents.modules.generic as generic\n",
    "import yaml\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    # æ–¹æ³•1: ä½¿ç”¨ generic.load_config (æ¨è)\n",
    "    # ä½†æˆ‘ä»¬å·²ç»æœ‰ base_config.yamlï¼Œç”¨æ–¹æ³•2\n",
    "    \n",
    "    # æ–¹æ³•2: æ‰‹åŠ¨åŠ è½½é…ç½® + get_environment\n",
    "    with open('base_config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    env_type = config['env']['type']\n",
    "    print(f\"ç¯å¢ƒç±»å‹: {env_type}\")\n",
    "    \n",
    "    # âœ… ä½¿ç”¨ get_environment å·¥å‚å‡½æ•°\n",
    "    env_class = alfworld.agents.environment.get_environment(env_type)\n",
    "    env = env_class(config, train_eval='eval_out_of_distribution')\n",
    "    env = env.init_env(batch_size=1)\n",
    "    \n",
    "    # æµ‹è¯•é‡ç½®\n",
    "    obs, info = env.reset()\n",
    "    print(\"âœ“ ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ\\n\")\n",
    "    \n",
    "    print(f\"åˆå§‹è§‚å¯Ÿ (å‰300å­—ç¬¦):\")\n",
    "    print(obs[0][:300])\n",
    "    print(\"...\\n\")\n",
    "    \n",
    "    # æ£€æŸ¥å¤‡é€‰åŠ¨ä½œ\n",
    "    if 'admissible_commands' in info:\n",
    "        admissible = info['admissible_commands'][0]\n",
    "        print(f\"âœ“ å¯ç”¨åŠ¨ä½œæ•°: {len(admissible)}\")\n",
    "        print(f\"  ç¤ºä¾‹åŠ¨ä½œ:\")\n",
    "        for i, action in enumerate(admissible[:5]):\n",
    "            print(f\"    {i+1}. {action}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æœªæä¾› admissible_commands\")\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\nâœ“ AlfWorld ç¯å¢ƒæµ‹è¯•é€šè¿‡\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. å®éªŒå‚æ•°é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ========== é…ç½® ==========\n",
    "NUM_TRIALS = 3           # å»ºè®®: 2-5\n",
    "NUM_ENVS = 5             # å»ºè®®: 5-20\n",
    "USE_MEMORY = True        # Reflexion å¼€å…³\n",
    "MODEL = \"qwen-7b-chat\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_NAME = f\"qwen_{timestamp}\"\n",
    "\n",
    "IS_RESUME = False\n",
    "RESUME_DIR = \"\"\n",
    "START_TRIAL_NUM = 0\n",
    "\n",
    "# ========== æ˜¾ç¤º ==========\n",
    "print(\"ğŸ“‹ å®éªŒé…ç½®\")\n",
    "print(\"=\"*60)\n",
    "print(f\"åç§°: {RUN_NAME}\")\n",
    "print(f\"Trials: {NUM_TRIALS}\")\n",
    "print(f\"ç¯å¢ƒæ•°: {NUM_ENVS}\")\n",
    "print(f\"Reflexion: {'âœ“' if USE_MEMORY else 'âœ—'}\")\n",
    "print(f\"æ¨¡å‹: {MODEL}\")\n",
    "print(f\"æ•°æ®: {DATA_DIR}\")\n",
    "print(f\"ç»“æœ: {RESULTS_DIR}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ—¶é—´ä¼°ç®—\n",
    "time_min = NUM_TRIALS * NUM_ENVS * 2\n",
    "time_max = NUM_TRIALS * NUM_ENVS * 5\n",
    "\n",
    "print(f\"\\nâ±ï¸  é¢„ä¼°: {time_min}-{time_max} åˆ†é’Ÿ\")\n",
    "print(f\"ğŸ’° æˆæœ¬: $0\")\n",
    "\n",
    "if time_max > 90:\n",
    "    print(\"\\nâš ï¸  è¶…è¿‡90åˆ†é’Ÿï¼ŒColab å¯èƒ½è¶…æ—¶\")\n",
    "    print(\"   å»ºè®®å‡å°‘ç¯å¢ƒæ•°é‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. è¿è¡Œå®éªŒ ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from alfworld_trial_qwen import run_trial\n",
    "from generate_reflections_qwen import update_memory\n",
    "\n",
    "def run_experiment():\n",
    "    # åˆå§‹åŒ–\n",
    "    if IS_RESUME:\n",
    "        logging_dir = RESUME_DIR\n",
    "        config_path = os.path.join(RESUME_DIR, f'env_results_trial_{START_TRIAL_NUM-1}.json')\n",
    "        with open(config_path) as f:\n",
    "            env_configs = json.load(f)\n",
    "        print(f\"âœ“ æ¢å¤è‡ª trial {START_TRIAL_NUM}\\n\")\n",
    "    else:\n",
    "        logging_dir = os.path.join(RESULTS_DIR, RUN_NAME)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "        env_configs = [{\n",
    "            'name': f'env_{i}',\n",
    "            'memory': [],\n",
    "            'is_success': False,\n",
    "            'skip': False\n",
    "        } for i in range(NUM_ENVS)]\n",
    "        print(f\"âœ“ åˆ›å»ºç›®å½•: {logging_dir}\\n\")\n",
    "    \n",
    "    world_log = os.path.join(logging_dir, 'world.log')\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸš€ å¼€å§‹å®éªŒ (Qwen 7B)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    progress = tqdm(total=NUM_TRIALS*NUM_ENVS, desc=\"æ€»è¿›åº¦\")\n",
    "    stats = []\n",
    "    \n",
    "    trial_idx = START_TRIAL_NUM if IS_RESUME else 0\n",
    "    \n",
    "    while trial_idx < NUM_TRIALS:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“ Trial {trial_idx+1}/{NUM_TRIALS}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        with open(world_log, 'a') as f:\n",
    "            f.write(f'\\n\\n***** Trial #{trial_idx} *****\\n\\n')\n",
    "        \n",
    "        trial_log = os.path.join(logging_dir, f'trial_{trial_idx}.log')\n",
    "        trial_config = os.path.join(logging_dir, f'env_results_trial_{trial_idx}.json')\n",
    "        \n",
    "        # è¿è¡Œ\n",
    "        print(f\"â³ è¿è¡Œ...\")\n",
    "        env_configs = run_trial(\n",
    "            trial_log, world_log, trial_idx,\n",
    "            env_configs, USE_MEMORY, MODEL\n",
    "        )\n",
    "        \n",
    "        progress.update(NUM_ENVS)\n",
    "        \n",
    "        # åæ€\n",
    "        if USE_MEMORY:\n",
    "            print(f\"\\nğŸ§  ç”Ÿæˆåæ€...\")\n",
    "            env_configs = update_memory(trial_log, env_configs)\n",
    "        \n",
    "        # ä¿å­˜\n",
    "        with open(trial_config, 'w') as f:\n",
    "            json.dump(env_configs, f, indent=4)\n",
    "        \n",
    "        with open(world_log, 'a') as f:\n",
    "            f.write(f'\\n\\n***** End Trial #{trial_idx} *****\\n\\n')\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        success = sum(1 for e in env_configs if e['is_success'])\n",
    "        acc = success / NUM_ENVS\n",
    "        \n",
    "        stats.append({\n",
    "            'trial': trial_idx,\n",
    "            'success': success,\n",
    "            'total': NUM_ENVS,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâœ“ Trial {trial_idx} å®Œæˆ\")\n",
    "        print(f\"   æˆåŠŸ: {success}/{NUM_ENVS} ({acc:.1%})\")\n",
    "        \n",
    "        if len(stats) > 1:\n",
    "            delta = acc - stats[-2]['accuracy']\n",
    "            trend = \"ğŸ“ˆ\" if delta > 0 else \"ğŸ“‰\" if delta < 0 else \"â¡ï¸\"\n",
    "            print(f\"   è¶‹åŠ¿: {trend} {delta:+.1%}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   GPU: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "        \n",
    "        trial_idx += 1\n",
    "    \n",
    "    progress.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ å®éªŒå®Œæˆ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nğŸ“ ç»“æœ: {logging_dir}\")\n",
    "    print(f\"ğŸ’¾ å·²åŒæ­¥åˆ° Google Drive\")\n",
    "    \n",
    "    return logging_dir, env_configs, stats\n",
    "\n",
    "# è¿è¡Œ\n",
    "try:\n",
    "    logging_dir, configs, stats = run_experiment()\n",
    "    print(\"\\nâœ“ æˆåŠŸ\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâš ï¸  ä¸­æ–­\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ç»“æœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(stats)\n",
    "df.columns = ['Trial', 'Success', 'Total', 'Accuracy']\n",
    "\n",
    "print(\"\\nğŸ“Š Qwen 7B å®éªŒç»“æœ\")\n",
    "print(\"=\"*60)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(\"\\nç»Ÿè®¡:\")\n",
    "    print(f\"  æ¨¡å‹: Qwen 7B (8-bit)\")\n",
    "    print(f\"  åˆå§‹: {df.iloc[0]['Accuracy']:.1%}\")\n",
    "    print(f\"  æœ€ç»ˆ: {df.iloc[-1]['Accuracy']:.1%}\")\n",
    "    print(f\"  æå‡: {(df.iloc[-1]['Accuracy']-df.iloc[0]['Accuracy']):.1%}\")\n",
    "    print(f\"  å¹³å‡: {df['Accuracy'].mean():.1%}\")\n",
    "    print(f\"  æœ€é«˜: {df['Accuracy'].max():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# å­¦ä¹ æ›²çº¿\n",
    "axes[0].plot(df['Trial'], df['Accuracy']*100, 'o-', lw=2.5, ms=8, color='#2E86AB')\n",
    "axes[0].fill_between(df['Trial'], 0, df['Accuracy']*100, alpha=0.2, color='#2E86AB')\n",
    "axes[0].set_xlabel('Trial', fontsize=12, weight='bold')\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12, weight='bold')\n",
    "axes[0].set_title('Qwen 7B Learning Curve', fontsize=14, weight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_ylim([0, 105])\n",
    "\n",
    "# æˆåŠŸæ•°\n",
    "colors = ['#06A77D' if i==len(df)-1 else '#A23B72' for i in range(len(df))]\n",
    "axes[1].bar(df['Trial'], df['Success'], color=colors, alpha=0.8)\n",
    "axes[1].set_xlabel('Trial', fontsize=12, weight='bold')\n",
    "axes[1].set_ylabel('Successes', fontsize=12, weight='bold')\n",
    "axes[1].set_title('Success Count', fontsize=14, weight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "save_path = os.path.join(logging_dir, 'qwen_curve.png')\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nâœ“ å›¾è¡¨: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. æŸ¥çœ‹åæ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_memory(run_dir, trial, env):\n",
    "    path = os.path.join(run_dir, f'env_results_trial_{trial}.json')\n",
    "    with open(path) as f:\n",
    "        configs = json.load(f)\n",
    "    \n",
    "    e = configs[env]\n",
    "    print(f\"ğŸ§  ç¯å¢ƒ #{env} (Trial {trial})\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"çŠ¶æ€: {'âœ“' if e['is_success'] else 'âœ—'}\")\n",
    "    print(f\"åæ€æ•°: {len(e['memory'])}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, m in enumerate(e['memory']):\n",
    "        print(f\"\\n--- åæ€ #{i+1} ---\")\n",
    "        print(m)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "if USE_MEMORY and NUM_TRIALS > 0:\n",
    "    view_memory(logging_dir, NUM_TRIALS-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. æ€§èƒ½ç»Ÿè®¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nâš¡ æ€§èƒ½\")\n",
    "print(\"=\"*60)\n",
    "print(f\"æ¨¡å‹: Qwen 7B (8-bit)\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Trials: {NUM_TRIALS}\")\n",
    "print(f\"ç¯å¢ƒ: {NUM_TRIALS * NUM_ENVS}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU:\")\n",
    "    print(f\"  å³°å€¼: {torch.cuda.max_memory_allocated()/1024**3:.2f}GB\")\n",
    "    print(f\"  å½“å‰: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "print(f\"\\nğŸ’° æˆæœ¬: $0\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. ä¸‹è½½ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "zip_name = f\"{RUN_NAME}.zip\"\n",
    "zip_path = os.path.join(RESULTS_DIR, zip_name)\n",
    "\n",
    "print(\"å‹ç¼©...\")\n",
    "shutil.make_archive(\n",
    "    os.path.join(RESULTS_DIR, RUN_NAME),\n",
    "    'zip',\n",
    "    logging_dir\n",
    ")\n",
    "print(f\"âœ“ å®Œæˆ: {zip_name}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"\\nä¸‹è½½...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"âœ“ å®Œæˆ\")\n",
    "else:\n",
    "    print(f\"\\nä½ç½®: {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š è¯´æ˜\n",
    "\n",
    "### å®Œæ•´å®ç°\n",
    "\n",
    "âœ… æŒ‰ç…§å®˜æ–¹æŒ‡å—é…ç½®  \n",
    "âœ… ä¸‹è½½æ‰€æœ‰å¿…éœ€çš„æ•°æ®æ–‡ä»¶  \n",
    "âœ… ä½¿ç”¨åŸå§‹ prompts  \n",
    "âœ… å®Œå…¨å…¼å®¹ OpenAI API  \n",
    "âœ… æ”¯æŒå®Œæ•´çš„ Reflexion æµç¨‹\n",
    "\n",
    "### ç›®å½•ç»“æ„\n",
    "\n",
    "```\n",
    "Google Drive/MyDrive/AlfWorld_Qwen/\n",
    "â”œâ”€â”€ models/\n",
    "â”‚   â””â”€â”€ Qwen-7B-Chat/         # 14GB\n",
    "â”œâ”€â”€ alfworld_data/             # 2GB\n",
    "â”‚   â”œâ”€â”€ json_2.1.1/\n",
    "â”‚   â””â”€â”€ logic/\n",
    "â”œâ”€â”€ reflexion/\n",
    "â”‚   â””â”€â”€ alfworld_runs/\n",
    "â”‚       â”œâ”€â”€ prompts/           # åŸå§‹ prompts\n",
    "â”‚       â”œâ”€â”€ utils_qwen.py\n",
    "â”‚       â”œâ”€â”€ alfworld_trial_qwen.py\n",
    "â”‚       â””â”€â”€ generate_reflections_qwen.py\n",
    "â””â”€â”€ results/\n",
    "    â””â”€â”€ qwen_YYYYMMDD_HHMMSS/\n",
    "```\n",
    "\n",
    "### ä¸å®˜æ–¹è®¾ç½®å¯¹æ¯”\n",
    "\n",
    "| æ­¥éª¤ | å®˜æ–¹ | æœ¬å®ç° |\n",
    "|------|------|--------|\n",
    "| å®‰è£… AlfWorld | âœ“ pip install alfworld[full] | âœ“ ç›¸åŒ |\n",
    "| ä¸‹è½½æ•°æ® | âœ“ alfworld-download | âœ“ ç›¸åŒ |\n",
    "| è®¾ç½®ç¯å¢ƒå˜é‡ | âœ“ ALFWORLD_DATA | âœ“ ç›¸åŒ |\n",
    "| ä½¿ç”¨ prompts | âœ“ alfworld_3prompts.json | âœ“ ç›¸åŒ |\n",
    "| API | OpenAI | Qwen (å…¼å®¹) |\n",
    "\n",
    "### æ€§èƒ½\n",
    "\n",
    "- **å‡†ç¡®æ€§**: ä¸ GPT-3.5 ç›¸è¿‘\n",
    "- **é€Ÿåº¦**: çº¦ 2-3 å€æ…¢\n",
    "- **æˆæœ¬**: $0 vs $50-150\n",
    "- **éšç§**: å®Œå…¨æœ¬åœ°\n",
    "\n",
    "### é—®é¢˜æ’æŸ¥\n",
    "\n",
    "**æ•°æ®æ–‡ä»¶ç¼ºå¤±**:\n",
    "```bash\n",
    "!alfworld-download --path {DATA_DIR}\n",
    "```\n",
    "\n",
    "**ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥**:\n",
    "æ£€æŸ¥ `ALFWORLD_DATA` ç¯å¢ƒå˜é‡\n",
    "\n",
    "**æ¨¡å‹æ¨ç†æ…¢**:\n",
    "- ç¡®è®¤ä½¿ç”¨ GPU\n",
    "- è€ƒè™‘ 4-bit é‡åŒ–\n",
    "- å‡å°‘ç¯å¢ƒæ•°é‡"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "alfworld_qwen_colab_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
