{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlfWorld DQN Training - Google Colab ğŸ¤–\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
    "\n",
    "ä½¿ç”¨ **Double DQN + DistilBERT** è®­ç»ƒ AlfWorld agent\n",
    "\n",
    "## ğŸ“‹ å®éªŒæ¦‚è¿°\n",
    "\n",
    "- **æ¨¡å‹**: DistilBERT-base-cased (Embeddingå±‚)\n",
    "- **ç®—æ³•**: Double DQN with Prioritized Replay\n",
    "- **ç¯å¢ƒ**: AlfWorld TextWorld ç¯å¢ƒ\n",
    "- **æ¶æ„**: å®Œå…¨æŒ‰ç…§ alfworld_meta_dqn-master å®ç°\n",
    "\n",
    "## ğŸ¯ ç‰¹ç‚¹\n",
    "\n",
    "- âœ… è½»é‡çº§ï¼ˆDistilBERT ~300MBï¼‰\n",
    "- âœ… å®Œæ•´çš„ DQN æ¶æ„ï¼ˆAttention + Recurrentï¼‰\n",
    "- âœ… Prioritized Experience Replay\n",
    "- âœ… å…è´¹è¿è¡Œï¼ˆColab T4 GPUï¼‰\n",
    "\n",
    "## â±ï¸ é¢„è®¡æ—¶é—´\n",
    "\n",
    "- ç¯å¢ƒæ­å»º: ~15 åˆ†é’Ÿ\n",
    "- è®­ç»ƒ 100 episodes: ~2-3 å°æ—¶\n",
    "- å®Œæ•´è®­ç»ƒ (1000+ episodes): 1-2 å¤©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ£€æŸ¥è¿è¡Œç¯å¢ƒ ğŸ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# æ£€æŸ¥ Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"{'âœ“' if IN_COLAB else 'âœ—'} Google Colab: {IN_COLAB}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\\n\")\n",
    "\n",
    "# æ£€æŸ¥ GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"  CUDA: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âŒ æœªæ£€æµ‹åˆ° GPUï¼è¯·å¯ç”¨ GPU è¿è¡Œæ—¶\")\n",
    "    print(\"   è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU\")\n",
    "\n",
    "!free -h | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æŒ‚è½½ Google Drive ğŸ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# é¡¹ç›®ç›®å½•ç»“æ„\n",
    "PROJECT_DIR = '/content/drive/MyDrive/AlfWorld_DQN' if IN_COLAB else './alfworld_dqn_workspace'\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'alfworld_data')\n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR, 'results')\n",
    "CHECKPOINTS_DIR = os.path.join(PROJECT_DIR, 'checkpoints')\n",
    "\n",
    "for dir_path in [PROJECT_DIR, DATA_DIR, RESULTS_DIR, CHECKPOINTS_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"ç›®å½•ç»“æ„:\")\n",
    "print(f\"  é¡¹ç›®: {PROJECT_DIR}\")\n",
    "print(f\"  æ•°æ®: {DATA_DIR}\")\n",
    "print(f\"  ç»“æœ: {RESULTS_DIR}\")\n",
    "print(f\"  æ£€æŸ¥ç‚¹: {CHECKPOINTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å®‰è£…ä¾èµ–åŒ… ğŸ“¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"å®‰è£…ä¾èµ–åŒ…...\\n\")\n",
    "\n",
    "# 1. AlfWorld (å®Œæ•´ç‰ˆ)\n",
    "print(\"[1/4] å®‰è£… AlfWorld...\")\n",
    "!pip install -q alfworld[full]\n",
    "\n",
    "# 2. Transformers + PyTorch\n",
    "print(\"[2/4] å®‰è£… Transformers...\")\n",
    "!pip install -q transformers>=4.35.0 accelerate sentencepiece\n",
    "\n",
    "# 3. å…¶ä»–ä¾èµ–\n",
    "print(\"[3/4] å®‰è£…å…¶ä»–å·¥å…·...\")\n",
    "!pip install -q pandas matplotlib seaborn tqdm pyyaml\n",
    "\n",
    "# 4. éªŒè¯å®‰è£…\n",
    "print(\"[4/4] éªŒè¯å®‰è£…...\")\n",
    "import alfworld\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "print(\"  âœ“ alfworld\", alfworld.__version__)\n",
    "print(\"  âœ“ transformers\")\n",
    "print(\"  âœ“ torch\", torch.__version__)\n",
    "\n",
    "print(\"\\nâœ“ æ‰€æœ‰ä¾èµ–å·²å®‰è£…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ä¸‹è½½ AlfWorld æ•°æ® ğŸ—‚ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "os.environ['ALFWORLD_DATA'] = DATA_DIR\n",
    "print(f\"ALFWORLD_DATA = {DATA_DIR}\\n\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½\n",
    "data_check_path = os.path.join(DATA_DIR, 'json_2.1.1')\n",
    "\n",
    "if os.path.exists(data_check_path) and os.path.exists(os.path.join(DATA_DIR, 'logic')):\n",
    "    print(\"âœ“ AlfWorld æ•°æ®å·²å­˜åœ¨\")\n",
    "    !ls -lh {DATA_DIR}\n",
    "else:\n",
    "    print(\"æ­£åœ¨ä¸‹è½½ AlfWorld æ•°æ®...\")\n",
    "    print(\"é¢„è®¡å¤§å°: ~2GB\")\n",
    "    print(\"é¢„è®¡æ—¶é—´: 5-10 åˆ†é’Ÿ\\n\")\n",
    "    \n",
    "    !alfworld-download --path {DATA_DIR}\n",
    "    \n",
    "    print(\"\\nâœ“ AlfWorld æ•°æ®ä¸‹è½½å®Œæˆ\")\n",
    "    !ls -lh {DATA_DIR}\n",
    "\n",
    "# éªŒè¯æ•°æ®å®Œæ•´æ€§\n",
    "required_paths = [\n",
    "    os.path.join(DATA_DIR, 'json_2.1.1/train'),\n",
    "    os.path.join(DATA_DIR, 'json_2.1.1/valid_seen'),\n",
    "    os.path.join(DATA_DIR, 'json_2.1.1/valid_unseen'),\n",
    "    os.path.join(DATA_DIR, 'logic/alfred.pddl'),\n",
    "    os.path.join(DATA_DIR, 'logic/alfred.twl2')\n",
    "]\n",
    "\n",
    "print(\"\\néªŒè¯æ•°æ®å®Œæ•´æ€§:\")\n",
    "all_ok = True\n",
    "for path in required_paths:\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"  {'âœ“' if exists else 'âœ—'} {os.path.basename(path)}\")\n",
    "    all_ok = all_ok and exists\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nâœ“ æ‰€æœ‰æ•°æ®æ–‡ä»¶å®Œæ•´\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ æŸäº›æ•°æ®æ–‡ä»¶ç¼ºå¤±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. åˆ›å»ºé…ç½®æ–‡ä»¶ âš™ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# åˆ›å»º base_config.yaml (ç®€åŒ–ç‰ˆ)\n",
    "config = {\n",
    "    'dataset': {\n",
    "        'data_path': f\"{DATA_DIR}/json_2.1.1/train\",\n",
    "        'eval_id_data_path': f\"{DATA_DIR}/json_2.1.1/valid_seen\",\n",
    "        'eval_ood_data_path': f\"{DATA_DIR}/json_2.1.1/valid_unseen\",\n",
    "    },\n",
    "    'logic': {\n",
    "        'domain': f\"{DATA_DIR}/logic/alfred.pddl\",\n",
    "        'grammar': f\"{DATA_DIR}/logic/alfred.twl2\",\n",
    "    },\n",
    "    'env': {\n",
    "        'type': 'AlfredTWEnv',\n",
    "        'regen_game_files': False,\n",
    "        'domain_randomization': False,\n",
    "        'task_types': [1, 2, 3, 4, 5, 6],\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = os.path.join(PROJECT_DIR, 'base_config.yaml')\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"âœ“ é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}\")\n",
    "print(\"\\nå†…å®¹:\")\n",
    "!cat {config_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. åŠ è½½ DistilBERT æ¨¡å‹ ğŸ§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"åŠ è½½ DistilBERT (ä¸ meta-dqn ä¸€è‡´)...\\n\")\n",
    "\n",
    "# 1. Tokenizer\n",
    "print(\"[1/2] åŠ è½½ tokenizer...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "word2id = tokenizer.get_vocab()\n",
    "print(f\"âœ“ Tokenizer å®Œæˆ\")\n",
    "print(f\"  è¯è¡¨å¤§å°: {len(word2id)}\")\n",
    "\n",
    "# 2. BERT Model (åªä¿ç•™ embeddings å±‚)\n",
    "print(\"\\n[2/2] åŠ è½½ DistilBERT...\")\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# ç§»é™¤ä¸éœ€è¦çš„å±‚ï¼ˆä¸ meta-dqn ä¸€è‡´ï¼‰\n",
    "bert_model.transformer = None\n",
    "bert_model.encoder = None\n",
    "\n",
    "# å†»ç»“å‚æ•°\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ç§»åŠ¨åˆ° GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model = bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "print(f\"âœ“ DistilBERT åŠ è½½å®Œæˆ\")\n",
    "print(f\"  å‚æ•°é‡: {sum(p.numel() for p in bert_model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"  è®¾å¤‡: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æµ‹è¯• AlfWorld ç¯å¢ƒ ğŸ§ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æµ‹è¯• AlfWorld ç¯å¢ƒ...\\n\")\n",
    "\n",
    "import alfworld\n",
    "import alfworld.agents.environment\n",
    "import yaml\n",
    "\n",
    "try:\n",
    "    # åŠ è½½é…ç½®\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # åˆ›å»ºç¯å¢ƒ\n",
    "    env_type = config['env']['type']\n",
    "    print(f\"ç¯å¢ƒç±»å‹: {env_type}\")\n",
    "    \n",
    "    env_class = alfworld.agents.environment.get_environment(env_type)\n",
    "    env = env_class(config, train_eval='eval_out_of_distribution')\n",
    "    env = env.init_env(batch_size=1)\n",
    "    \n",
    "    # é‡ç½®ç¯å¢ƒ\n",
    "    obs, info = env.reset()\n",
    "    print(\"âœ“ ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ\\n\")\n",
    "    \n",
    "    print(f\"åˆå§‹è§‚å¯Ÿ (å‰200å­—ç¬¦):\")\n",
    "    print(obs[0][:200])\n",
    "    print(\"...\\n\")\n",
    "    \n",
    "    # æ£€æŸ¥ admissible commands\n",
    "    if 'admissible_commands' in info:\n",
    "        admissible = info['admissible_commands'][0]\n",
    "        print(f\"âœ“ å€™é€‰åŠ¨ä½œæ•°: {len(admissible)}\")\n",
    "        print(f\"  ç¤ºä¾‹åŠ¨ä½œ:\")\n",
    "        for i, action in enumerate(admissible[:5]):\n",
    "            print(f\"    {i+1}. {action}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\nâœ“ AlfWorld ç¯å¢ƒæµ‹è¯•é€šè¿‡\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DQN ç½‘ç»œæ¶æ„å®ç° ğŸ—ï¸\n",
    "\n",
    "åŒ…å«å®Œæ•´çš„ Layer ç»„ä»¶å’Œ Policy ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°† alfworld_dqn_train.py çš„ä»£ç åµŒå…¥åˆ° notebook\n",
    "# è¿™æ ·å¯ä»¥ç›´æ¥åœ¨ Colab ä¸­è¿è¡Œï¼Œæ— éœ€å¤–éƒ¨æ–‡ä»¶\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ======================== Layer Components ========================\n",
    "\n",
    "def compute_mask(x):\n",
    "    \"\"\"è®¡ç®— padding mask\"\"\"\n",
    "    mask = torch.ne(x, 0).float()\n",
    "    if x.is_cuda:\n",
    "        mask = mask.cuda()\n",
    "    return mask\n",
    "\n",
    "\n",
    "def masked_mean(x, m=None, dim=1):\n",
    "    \"\"\"å¸¦ mask çš„å¹³å‡æ± åŒ–\"\"\"\n",
    "    if m is None:\n",
    "        return torch.mean(x, dim=dim)\n",
    "    x = x * m.unsqueeze(-1)\n",
    "    mask_sum = torch.sum(m, dim=-1)\n",
    "    tmp = torch.eq(mask_sum, 0).float()\n",
    "    if x.is_cuda:\n",
    "        tmp = tmp.cuda()\n",
    "    mask_sum = mask_sum + tmp\n",
    "    res = torch.sum(x, dim=dim)\n",
    "    res = res / mask_sum.unsqueeze(-1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def masked_softmax(x, m=None, axis=-1):\n",
    "    \"\"\"å¸¦ mask çš„ softmax\"\"\"\n",
    "    x = torch.clamp(x, min=-15.0, max=15.0)\n",
    "    if m is not None:\n",
    "        m = m.float()\n",
    "        x = x * m\n",
    "    e_x = torch.exp(x - torch.max(x, dim=axis, keepdim=True)[0])\n",
    "    if m is not None:\n",
    "        e_x = e_x * m\n",
    "    softmax = e_x / (torch.sum(e_x, dim=axis, keepdim=True) + 1e-6)\n",
    "    return softmax\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head Self-Attention\"\"\"\n",
    "    def __init__(self, block_hidden_dim, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.block_hidden_dim = block_hidden_dim\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        assert block_hidden_dim % n_head == 0\n",
    "        self.d_k = block_hidden_dim // n_head\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(block_hidden_dim, block_hidden_dim) for _ in range(4)])\n",
    "\n",
    "    def forward(self, query, mask, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "                            for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores * mask + -1e9 * (1 - mask)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = torch.matmul(attn, value)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.block_hidden_dim)\n",
    "        out = self.linears[-1](out)\n",
    "\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "class CQAttention(nn.Module):\n",
    "    \"\"\"Context-Query Attention\"\"\"\n",
    "    def __init__(self, block_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        w = torch.empty(block_hidden_dim * 3)\n",
    "        nn.init.xavier_uniform_(w)\n",
    "        self.w = nn.Parameter(w)\n",
    "\n",
    "    def forward(self, C, Q, c_mask, q_mask):\n",
    "        batch_size, c_len, q_len = C.size(0), C.size(1), Q.size(1)\n",
    "\n",
    "        C_expanded = C.unsqueeze(2).expand(-1, -1, q_len, -1)\n",
    "        Q_expanded = Q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
    "        CQ = C_expanded * Q_expanded\n",
    "\n",
    "        S_inputs = torch.cat([C_expanded, Q_expanded, CQ], dim=-1)\n",
    "        S = torch.matmul(S_inputs, self.w)\n",
    "\n",
    "        S1 = masked_softmax(S, m=q_mask.unsqueeze(1), axis=2)\n",
    "        A = torch.bmm(S1, Q)\n",
    "\n",
    "        S2 = masked_softmax(S, m=c_mask.unsqueeze(2), axis=1)\n",
    "        S2_max, _ = torch.max(S2, dim=1, keepdim=True)\n",
    "        B = torch.bmm(S2_max, Q).expand(-1, c_len, -1)\n",
    "\n",
    "        out = torch.cat([C, A, C * A, C * B], dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def PosEncoder(x):\n",
    "    \"\"\"Position Encoding\"\"\"\n",
    "    batch_size, seq_len, d_model = x.size()\n",
    "    pos = torch.arange(0, seq_len).unsqueeze(0).expand(batch_size, -1).float()\n",
    "    if x.is_cuda:\n",
    "        pos = pos.cuda()\n",
    "\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "    if x.is_cuda:\n",
    "        div_term = div_term.cuda()\n",
    "\n",
    "    pe = torch.zeros(batch_size, seq_len, d_model)\n",
    "    if x.is_cuda:\n",
    "        pe = pe.cuda()\n",
    "\n",
    "    pe[:, :, 0::2] = torch.sin(pos.unsqueeze(-1) * div_term)\n",
    "    pe[:, :, 1::2] = torch.cos(pos.unsqueeze(-1) * div_term)\n",
    "\n",
    "    return x + pe\n",
    "\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"Depthwise Separable Convolution\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k, bias=True):\n",
    "        super().__init__()\n",
    "        self.depthwise_conv = nn.Conv1d(in_channels=in_ch, out_channels=in_ch, kernel_size=k, groups=in_ch, padding=k // 2, bias=False)\n",
    "        self.pointwise_conv = nn.Conv1d(in_channels=in_ch, out_channels=out_ch, kernel_size=1, padding=0, bias=bias)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Encoder Block\"\"\"\n",
    "    def __init__(self, conv_num, ch_num, k, block_hidden_dim, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.convs = nn.ModuleList([DepthwiseSeparableConv(ch_num, ch_num, k) for _ in range(conv_num)])\n",
    "        self.self_att = SelfAttention(block_hidden_dim, n_head, dropout)\n",
    "        self.FFN_1 = nn.Linear(ch_num, ch_num)\n",
    "        self.FFN_2 = nn.Linear(ch_num, ch_num)\n",
    "        self.norm_C = nn.ModuleList([nn.LayerNorm(block_hidden_dim) for _ in range(conv_num)])\n",
    "        self.norm_1 = nn.LayerNorm(block_hidden_dim)\n",
    "        self.norm_2 = nn.LayerNorm(block_hidden_dim)\n",
    "        self.conv_num = conv_num\n",
    "\n",
    "    def forward(self, x, mask, squared_mask, l, blks):\n",
    "        total_layers = (self.conv_num + 2) * blks\n",
    "        out = PosEncoder(x)\n",
    "        out = out * mask.unsqueeze(-1)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            res = out\n",
    "            out = self.norm_C[i](out)\n",
    "            if (i) % 2 == 0:\n",
    "                out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "            out = conv(out, mask)\n",
    "            out = self.layer_dropout(out, res, self.dropout * float(l) / total_layers)\n",
    "            out = out * mask.unsqueeze(-1)\n",
    "            l += 1\n",
    "\n",
    "        res = out\n",
    "        out = self.norm_1(out)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        out, _ = self.self_att(out, squared_mask, out, out)\n",
    "        out = self.layer_dropout(out, res, self.dropout * float(l) / total_layers)\n",
    "        out = out * mask.unsqueeze(-1)\n",
    "        l += 1\n",
    "\n",
    "        res = out\n",
    "        out = self.norm_2(out)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        out = self.FFN_1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.FFN_2(out)\n",
    "        out = self.layer_dropout(out, res, self.dropout * float(l) / total_layers)\n",
    "        out = out * mask.unsqueeze(-1)\n",
    "        l += 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    def layer_dropout(self, inputs, residual, dropout):\n",
    "        if self.training == True:\n",
    "            pred = torch.empty(1).uniform_(0, 1) < dropout\n",
    "            if pred:\n",
    "                return residual\n",
    "            else:\n",
    "                return F.dropout(inputs, dropout, training=self.training) + residual\n",
    "        else:\n",
    "            return inputs + residual\n",
    "\n",
    "\n",
    "print(\"âœ“ Layer ç»„ä»¶å·²å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Policy Network ========================\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"DQN Policy Network\"\"\"\n",
    "    def __init__(self, bert_model, word_vocab_size, config):\n",
    "        super().__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder_layers = config['encoder_layers']\n",
    "        self.encoder_conv_num = config['encoder_conv_num']\n",
    "        self.block_hidden_dim = config['block_hidden_dim']\n",
    "        self.n_heads = config['n_heads']\n",
    "        self.block_dropout = config['block_dropout']\n",
    "        self.dropout = config['dropout']\n",
    "        self.recurrent = config['recurrent']\n",
    "\n",
    "        self._def_layers()\n",
    "\n",
    "    def _def_layers(self):\n",
    "        BERT_EMBEDDING_SIZE = 768\n",
    "\n",
    "        self.word_embedding_prj = nn.Linear(BERT_EMBEDDING_SIZE, self.block_hidden_dim, bias=False)\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncoderBlock(\n",
    "                conv_num=self.encoder_conv_num,\n",
    "                ch_num=self.block_hidden_dim,\n",
    "                k=5,\n",
    "                block_hidden_dim=self.block_hidden_dim,\n",
    "                n_head=self.n_heads,\n",
    "                dropout=self.block_dropout\n",
    "            ) for _ in range(self.encoder_layers)\n",
    "        ])\n",
    "\n",
    "        self.aggregation_attention = CQAttention(block_hidden_dim=self.block_hidden_dim, dropout=self.dropout)\n",
    "        self.aggregation_attention_proj = nn.Linear(self.block_hidden_dim * 4, self.block_hidden_dim)\n",
    "\n",
    "        if self.recurrent:\n",
    "            self.rnncell = nn.GRUCell(self.block_hidden_dim, self.block_hidden_dim)\n",
    "            self.dynamics_aggregation = nn.Linear(self.block_hidden_dim * 2, self.block_hidden_dim)\n",
    "        else:\n",
    "            self.rnncell, self.dynamics_aggregation = None, None\n",
    "\n",
    "        self.action_scorer_linear_1 = nn.Linear(self.block_hidden_dim * 2, self.block_hidden_dim)\n",
    "        self.action_scorer_linear_2 = nn.Linear(self.block_hidden_dim, 1)\n",
    "        self.action_scorer_extra_self_attention = SelfAttention(self.block_hidden_dim, self.n_heads, self.dropout)\n",
    "        self.action_scorer_extra_linear = nn.Linear(self.block_hidden_dim, self.block_hidden_dim)\n",
    "\n",
    "    def get_bert_embeddings(self, _input_words, _input_masks):\n",
    "        if _input_words.size(1) > 512:\n",
    "            seg_length = 500\n",
    "            outputs = []\n",
    "            num_batch = (_input_words.size(1) + seg_length - 1) // seg_length\n",
    "            for i in range(num_batch):\n",
    "                batch_input = _input_words[:, i * seg_length: (i + 1) * seg_length]\n",
    "                batch_mask = _input_masks[:, i * seg_length: (i + 1) * seg_length]\n",
    "                out = self.get_bert_embeddings(batch_input, batch_mask)\n",
    "                outputs.append(out)\n",
    "            return torch.cat(outputs, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            res = self.bert_model.embeddings(_input_words)\n",
    "            res = res * _input_masks.unsqueeze(-1)\n",
    "        return res\n",
    "\n",
    "    def embed(self, input_words, input_word_masks):\n",
    "        word_embeddings = self.get_bert_embeddings(input_words, input_word_masks)\n",
    "        word_embeddings = word_embeddings * input_word_masks.unsqueeze(-1)\n",
    "        word_embeddings = self.word_embedding_prj(word_embeddings)\n",
    "        word_embeddings = word_embeddings * input_word_masks.unsqueeze(-1)\n",
    "        return word_embeddings\n",
    "\n",
    "    def encode_text(self, input_word_ids):\n",
    "        input_word_masks = compute_mask(input_word_ids)\n",
    "        embeddings = self.embed(input_word_ids, input_word_masks)\n",
    "\n",
    "        squared_mask = torch.bmm(input_word_masks.unsqueeze(-1), input_word_masks.unsqueeze(1))\n",
    "        encoding_sequence = embeddings\n",
    "\n",
    "        for i in range(self.encoder_layers):\n",
    "            encoding_sequence = self.encoder[i](\n",
    "                encoding_sequence, input_word_masks, squared_mask,\n",
    "                i * (self.encoder_conv_num + 2) + 1, self.encoder_layers\n",
    "            )\n",
    "\n",
    "        return encoding_sequence, input_word_masks\n",
    "\n",
    "    def aggretate_information(self, h_obs, obs_mask, h_td, td_mask):\n",
    "        aggregated_obs_representation = self.aggregation_attention(h_obs, h_td, obs_mask, td_mask)\n",
    "        aggregated_obs_representation = self.aggregation_attention_proj(aggregated_obs_representation)\n",
    "        aggregated_obs_representation = torch.tanh(aggregated_obs_representation)\n",
    "        aggregated_obs_representation = aggregated_obs_representation * obs_mask.unsqueeze(-1)\n",
    "        return aggregated_obs_representation\n",
    "\n",
    "    def masked_mean(self, h_obs, obs_mask):\n",
    "        _mask = torch.sum(obs_mask, -1)\n",
    "        obs_representations = torch.sum(h_obs, -2)\n",
    "        tmp = torch.eq(_mask, 0).float()\n",
    "        if obs_representations.is_cuda:\n",
    "            tmp = tmp.cuda()\n",
    "        _mask = _mask + tmp\n",
    "        obs_representations = obs_representations / _mask.unsqueeze(-1)\n",
    "        return obs_representations\n",
    "\n",
    "    def score_actions(self, candidate_representations, cand_mask, h_obs, obs_mask, current_dynamics, fix_shared_components=False):\n",
    "        batch_size, num_candidate = candidate_representations.size(0), candidate_representations.size(1)\n",
    "\n",
    "        aggregated_obs_representation = self.masked_mean(h_obs, obs_mask)\n",
    "\n",
    "        if self.recurrent:\n",
    "            aggregated_obs_representation = self.dynamics_aggregation(\n",
    "                torch.cat([aggregated_obs_representation, current_dynamics], -1)\n",
    "            )\n",
    "            aggregated_obs_representation = torch.relu(aggregated_obs_representation)\n",
    "\n",
    "        if fix_shared_components:\n",
    "            aggregated_obs_representation = aggregated_obs_representation.detach()\n",
    "            candidate_representations = candidate_representations.detach()\n",
    "\n",
    "        new_h_expanded = torch.stack([aggregated_obs_representation] * num_candidate, 1)\n",
    "\n",
    "        output = self.action_scorer_linear_1(\n",
    "            torch.cat([candidate_representations, new_h_expanded], -1)\n",
    "        )\n",
    "        output = torch.relu(output)\n",
    "        output = output * cand_mask.unsqueeze(-1)\n",
    "\n",
    "        if fix_shared_components:\n",
    "            cand_mask_squared = torch.bmm(cand_mask.unsqueeze(-1), cand_mask.unsqueeze(1))\n",
    "            output, _ = self.action_scorer_extra_self_attention(output, cand_mask_squared, output, output)\n",
    "            output = self.action_scorer_extra_linear(output)\n",
    "            output = torch.relu(output)\n",
    "            output = output * cand_mask.unsqueeze(-1)\n",
    "\n",
    "        output = self.action_scorer_linear_2(output).squeeze(-1)\n",
    "        output = output * cand_mask\n",
    "\n",
    "        return output, cand_mask\n",
    "\n",
    "\n",
    "print(\"âœ“ Policy ç½‘ç»œå·²å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Double DQN Agent å®ç° ğŸ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import torch.optim as optim\n",
    "\n",
    "Transition = namedtuple('Transition', [\n",
    "    'obs', 'task', 'action_idx', 'reward', 'next_obs', 'next_task', 'done',\n",
    "    'admissible', 'next_admissible', 'prev_dynamics'\n",
    "])\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized Experience Replay\"\"\"\n",
    "    def __init__(self, capacity=500000, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        max_priority = self.priorities[:self.size].max() if self.size > 0 else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(Transition(*args))\n",
    "        else:\n",
    "            self.buffer[self.position] = Transition(*args)\n",
    "\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if self.size == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        priorities = self.priorities[:self.size]\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(self.size, min(batch_size, self.size), p=probs, replace=False)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = self.size\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class DoubleDQNAgent:\n",
    "    \"\"\"Double DQN Agent\"\"\"\n",
    "    def __init__(self, online_net, target_net, tokenizer, word2id, device, config):\n",
    "        self.online_net = online_net.to(device)\n",
    "        self.target_net = target_net.to(device)\n",
    "        self.target_net.load_state_dict(online_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2id = word2id\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.gamma = config['gamma']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.target_update_frequency = config['target_update_frequency']\n",
    "        self.update_per_k_game_steps = config['update_per_k_game_steps']\n",
    "        self.multi_step = config.get('multi_step', 3)\n",
    "\n",
    "        self.epsilon_start = config['epsilon_start']\n",
    "        self.epsilon_end = config['epsilon_end']\n",
    "        self.epsilon_anneal_episodes = config['epsilon_anneal_episodes']\n",
    "        self.epsilon = self.epsilon_start\n",
    "\n",
    "        self.optimizer = optim.Adam(online_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = PrioritizedReplayBuffer(capacity=config['replay_memory_capacity'])\n",
    "\n",
    "        self.step_in_total = 0\n",
    "        self.episode_no = 0\n",
    "        self.mode = \"train\"\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.episode_no < self.epsilon_anneal_episodes:\n",
    "            self.epsilon = self.epsilon_start - (self.epsilon_start - self.epsilon_end) * (self.episode_no / self.epsilon_anneal_episodes)\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_end\n",
    "\n",
    "    def words_to_ids(self, words):\n",
    "        ids = self.tokenizer.encode(words, add_special_tokens=True)\n",
    "        return torch.tensor([ids], dtype=torch.long).to(self.device)\n",
    "\n",
    "    def encode(self, texts, use_model=\"online\"):\n",
    "        model = self.online_net if use_model == \"online\" else self.target_net\n",
    "\n",
    "        input_ids_list = []\n",
    "        for text in texts:\n",
    "            ids = self.tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "            input_ids_list.append(ids)\n",
    "\n",
    "        max_len = max(len(ids) for ids in input_ids_list)\n",
    "        input_ids_padded = [ids + [0] * (max_len - len(ids)) for ids in input_ids_list]\n",
    "        input_ids = torch.tensor(input_ids_padded, dtype=torch.long).to(self.device)\n",
    "\n",
    "        h, mask = model.encode_text(input_ids)\n",
    "        return h, mask\n",
    "\n",
    "    def choose_random_action(self, action_scores, action_candidate_list):\n",
    "        batch_size = action_scores.size(0)\n",
    "        indices = []\n",
    "        for j in range(batch_size):\n",
    "            indices.append(np.random.choice(len(action_candidate_list[j])))\n",
    "        return np.array(indices)\n",
    "\n",
    "    def choose_maxQ_action(self, action_scores, action_mask=None):\n",
    "        action_scores = action_scores - torch.min(action_scores, -1, keepdim=True)[0] + 1e-2\n",
    "        if action_mask is not None:\n",
    "            action_scores = action_scores * action_mask\n",
    "        action_indices = torch.argmax(action_scores, -1)\n",
    "        return action_indices.cpu().numpy()\n",
    "\n",
    "    def action_scoring(self, action_candidate_list, h_obs, obs_mask, h_td, td_mask, previous_dynamics, use_model=\"online\"):\n",
    "        model = self.online_net if use_model == \"online\" else self.target_net\n",
    "        batch_size = len(action_candidate_list)\n",
    "\n",
    "        aggregated_obs_representation = model.aggretate_information(h_obs, obs_mask, h_td, td_mask)\n",
    "\n",
    "        if model.recurrent:\n",
    "            averaged_representation = model.masked_mean(aggregated_obs_representation, obs_mask)\n",
    "            current_dynamics = model.rnncell(averaged_representation, previous_dynamics) if previous_dynamics is not None else model.rnncell(averaged_representation)\n",
    "        else:\n",
    "            current_dynamics = None\n",
    "\n",
    "        max_num_candidate = max(len(candidates) for candidates in action_candidate_list)\n",
    "        candidate_representations_list = []\n",
    "        candidate_masks_list = []\n",
    "\n",
    "        for candidates in action_candidate_list:\n",
    "            if len(candidates) == 0:\n",
    "                candidate_reps = torch.zeros(max_num_candidate, model.block_hidden_dim).to(self.device)\n",
    "                candidate_mask = torch.zeros(max_num_candidate).to(self.device)\n",
    "            else:\n",
    "                h_cand, cand_mask = self.encode(candidates, use_model=use_model)\n",
    "                candidate_reps = model.masked_mean(h_cand, cand_mask)\n",
    "\n",
    "                if len(candidates) < max_num_candidate:\n",
    "                    padding = torch.zeros(max_num_candidate - len(candidates), model.block_hidden_dim).to(self.device)\n",
    "                    candidate_reps = torch.cat([candidate_reps, padding], dim=0)\n",
    "\n",
    "                    mask_padding = torch.zeros(max_num_candidate - len(candidates)).to(self.device)\n",
    "                    candidate_mask = torch.cat([cand_mask.sum(dim=1) > 0, mask_padding.bool()], dim=0).float()\n",
    "                else:\n",
    "                    candidate_mask = (cand_mask.sum(dim=1) > 0).float()\n",
    "\n",
    "            candidate_representations_list.append(candidate_reps)\n",
    "            candidate_masks_list.append(candidate_mask)\n",
    "\n",
    "        candidate_representations = torch.stack(candidate_representations_list)\n",
    "        candidate_masks = torch.stack(candidate_masks_list)\n",
    "\n",
    "        action_scores, action_masks = model.score_actions(\n",
    "            candidate_representations, candidate_masks,\n",
    "            aggregated_obs_representation, obs_mask,\n",
    "            current_dynamics, fix_shared_components=False\n",
    "        )\n",
    "\n",
    "        return action_scores, action_masks, current_dynamics\n",
    "\n",
    "    def admissible_commands_act(self, obs_strings, task_strings, action_candidate_list, previous_dynamics, random=False):\n",
    "        with torch.no_grad():\n",
    "            if self.mode == \"eval\" or not random:\n",
    "                h_obs, obs_mask = self.encode(obs_strings, use_model=\"online\")\n",
    "                h_td, td_mask = self.encode(task_strings, use_model=\"online\")\n",
    "                action_scores, action_masks, current_dynamics = self.action_scoring(\n",
    "                    action_candidate_list, h_obs, obs_mask, h_td, td_mask, previous_dynamics, use_model=\"online\"\n",
    "                )\n",
    "                action_indices = self.choose_maxQ_action(action_scores, action_masks)\n",
    "                chosen_actions = [candidates[idx] for candidates, idx in zip(action_candidate_list, action_indices)]\n",
    "                return chosen_actions, action_indices, current_dynamics\n",
    "\n",
    "            batch_size = len(obs_strings)\n",
    "            h_obs, obs_mask = self.encode(obs_strings, use_model=\"online\")\n",
    "            h_td, td_mask = self.encode(task_strings, use_model=\"online\")\n",
    "            action_scores, action_masks, current_dynamics = self.action_scoring(\n",
    "                action_candidate_list, h_obs, obs_mask, h_td, td_mask, previous_dynamics, use_model=\"online\"\n",
    "            )\n",
    "\n",
    "            action_indices_maxq = self.choose_maxQ_action(action_scores, action_masks)\n",
    "            action_indices_random = self.choose_random_action(action_scores, action_candidate_list)\n",
    "\n",
    "            rand_num = np.random.uniform(low=0.0, high=1.0, size=(batch_size,))\n",
    "            less_than_epsilon = (rand_num < self.epsilon).astype(\"float32\")\n",
    "            greater_than_epsilon = 1.0 - less_than_epsilon\n",
    "\n",
    "            chosen_indices = less_than_epsilon * action_indices_random + greater_than_epsilon * action_indices_maxq\n",
    "            chosen_indices = chosen_indices.astype(int)\n",
    "            chosen_actions = [candidates[idx] for candidates, idx in zip(action_candidate_list, chosen_indices)]\n",
    "\n",
    "            return chosen_actions, chosen_indices, current_dynamics\n",
    "\n",
    "    def update(self, beta=0.4):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        transitions, indices, weights = self.memory.sample(self.batch_size, beta=beta)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        weights = torch.FloatTensor(weights).to(self.device)\n",
    "\n",
    "        batch_size = len(transitions)\n",
    "\n",
    "        h_obs, obs_mask = self.encode(batch.obs, use_model=\"online\")\n",
    "        h_td, td_mask = self.encode(batch.task, use_model=\"online\")\n",
    "        action_scores, _, current_dynamics = self.action_scoring(\n",
    "            batch.admissible, h_obs, obs_mask, h_td, td_mask, batch.prev_dynamics, use_model=\"online\"\n",
    "        )\n",
    "\n",
    "        q_values = []\n",
    "        for i in range(batch_size):\n",
    "            q_values.append(action_scores[i, batch.action_idx[i]])\n",
    "        q_values = torch.stack(q_values)\n",
    "\n",
    "        next_q_values = []\n",
    "        for i in range(batch_size):\n",
    "            if batch.done[i]:\n",
    "                next_q_values.append(torch.tensor(0.0).to(self.device))\n",
    "            else:\n",
    "                h_obs_next, obs_mask_next = self.encode([batch.next_obs[i]], use_model=\"online\")\n",
    "                h_td_next, td_mask_next = self.encode([batch.next_task[i]], use_model=\"online\")\n",
    "\n",
    "                aggregated_obs_next = self.online_net.aggretate_information(h_obs_next, obs_mask_next, h_td_next, td_mask_next)\n",
    "                if self.online_net.recurrent:\n",
    "                    averaged_next = self.online_net.masked_mean(aggregated_obs_next, obs_mask_next)\n",
    "                    next_dynamics = self.online_net.rnncell(averaged_next, current_dynamics[i:i+1])\n",
    "                else:\n",
    "                    next_dynamics = None\n",
    "\n",
    "                action_scores_online, _, _ = self.action_scoring(\n",
    "                    [batch.next_admissible[i]], h_obs_next, obs_mask_next, h_td_next, td_mask_next,\n",
    "                    next_dynamics, use_model=\"online\"\n",
    "                )\n",
    "                best_action_idx = action_scores_online[0].argmax().item()\n",
    "\n",
    "                action_scores_target, _, _ = self.action_scoring(\n",
    "                    [batch.next_admissible[i]], h_obs_next, obs_mask_next, h_td_next, td_mask_next,\n",
    "                    next_dynamics, use_model=\"target\"\n",
    "                )\n",
    "                next_q_values.append(action_scores_target[0, best_action_idx])\n",
    "\n",
    "        next_q_values = torch.stack(next_q_values)\n",
    "\n",
    "        rewards = torch.tensor(batch.reward, dtype=torch.float32).to(self.device)\n",
    "        target_q_values = rewards + (self.gamma ** self.multi_step) * next_q_values\n",
    "\n",
    "        td_errors = torch.abs(q_values - target_q_values).detach().cpu().numpy()\n",
    "        self.memory.update_priorities(indices, td_errors + 1e-6)\n",
    "\n",
    "        loss = (weights * F.smooth_l1_loss(q_values, target_q_values, reduction='none')).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), max_norm=0.1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "\n",
    "print(\"âœ“ DoubleDQNAgent å·²å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. è®­ç»ƒå¾ªç¯å®ç° ğŸ”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def process_ob(ob):\n",
    "    \"\"\"å¤„ç†è§‚å¯Ÿ\"\"\"\n",
    "    if ob.startswith('You arrive at loc '):\n",
    "        ob = ob[ob.find('. ')+2:]\n",
    "    return ob\n",
    "\n",
    "\n",
    "def run_episode(env, agent, task_desc, max_steps=50, train=True):\n",
    "    \"\"\"è¿è¡Œä¸€ä¸ª episode\"\"\"\n",
    "    obs, info = env.reset()\n",
    "    obs_text = '\\n'.join(obs[0].split('\\n\\n')[1:])\n",
    "    obs_text = process_ob(obs_text)\n",
    "\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    losses = []\n",
    "    prev_dynamics = None\n",
    "\n",
    "    while step < max_steps:\n",
    "        if 'admissible_commands' not in info or len(info['admissible_commands']) == 0:\n",
    "            break\n",
    "\n",
    "        admissible = info['admissible_commands'][0]\n",
    "\n",
    "        actions, action_indices, current_dynamics = agent.admissible_commands_act(\n",
    "            [obs_text], [task_desc], [admissible], prev_dynamics, random=train\n",
    "        )\n",
    "        action = actions[0]\n",
    "        action_idx = action_indices[0]\n",
    "\n",
    "        next_obs, _, done, next_info = env.step([action])\n",
    "        next_obs_text = process_ob(next_obs[0])\n",
    "        reward = 1.0 if next_info['won'][0] else 0.0\n",
    "        done = done[0]\n",
    "\n",
    "        if train:\n",
    "            next_admissible = next_info.get('admissible_commands', [[]])[0] if not done else []\n",
    "            agent.memory.push(\n",
    "                obs_text, task_desc, action_idx, reward, next_obs_text, task_desc,\n",
    "                done, admissible, next_admissible, prev_dynamics\n",
    "            )\n",
    "\n",
    "            if agent.step_in_total % agent.update_per_k_game_steps == 0 and len(agent.memory) >= agent.batch_size:\n",
    "                loss = agent.update(beta=0.4)\n",
    "                if loss is not None:\n",
    "                    losses.append(loss)\n",
    "\n",
    "        total_reward += reward\n",
    "        obs_text = next_obs_text\n",
    "        info = next_info\n",
    "        prev_dynamics = current_dynamics\n",
    "        step += 1\n",
    "        agent.step_in_total += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    agent.episode_no += 1\n",
    "    agent.update_epsilon()\n",
    "\n",
    "    if agent.episode_no % agent.target_update_frequency == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    avg_loss = np.mean(losses) if losses else 0.0\n",
    "    return total_reward, step, avg_loss, done\n",
    "\n",
    "\n",
    "print(\"âœ“ è®­ç»ƒå‡½æ•°å·²å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. è®­ç»ƒé…ç½® âš™ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ========== è®­ç»ƒå‚æ•° ==========\n",
    "NUM_EPISODES = 100        # å¿«é€Ÿæµ‹è¯•ï¼Œå®Œæ•´è®­ç»ƒå»ºè®® 1000-5000\n",
    "EVAL_FREQ = 20            # æ¯ 20 episodes è¯„ä¼°ä¸€æ¬¡\n",
    "SAVE_FREQ = 50            # æ¯ 50 episodes ä¿å­˜ä¸€æ¬¡\n",
    "\n",
    "# ========== æ¨¡å‹é…ç½® ==========\n",
    "MODEL_CONFIG = {\n",
    "    'encoder_layers': 1,\n",
    "    'encoder_conv_num': 5,\n",
    "    'block_hidden_dim': 64,\n",
    "    'n_heads': 1,\n",
    "    'block_dropout': 0.1,\n",
    "    'dropout': 0.1,\n",
    "    'recurrent': True,\n",
    "}\n",
    "\n",
    "# ========== DQN é…ç½® ==========\n",
    "TRAINING_CONFIG = {\n",
    "    'learning_rate': 0.001,\n",
    "    'gamma': 0.9,\n",
    "    'batch_size': 64,\n",
    "    'target_update_frequency': 500,\n",
    "    'update_per_k_game_steps': 5,\n",
    "    'multi_step': 3,\n",
    "    'epsilon_start': 0.3,\n",
    "    'epsilon_end': 0.1,\n",
    "    'epsilon_anneal_episodes': 1000,\n",
    "    'replay_memory_capacity': 100000,  # å‡å°‘ä»¥èŠ‚çœå†…å­˜\n",
    "}\n",
    "\n",
    "# ========== è·¯å¾„ ==========\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_NAME = f\"dqn_distilbert_{timestamp}\"\n",
    "SAVE_PATH = os.path.join(CHECKPOINTS_DIR, f\"{RUN_NAME}.pt\")\n",
    "LOG_PATH = os.path.join(RESULTS_DIR, f\"{RUN_NAME}_log.txt\")\n",
    "\n",
    "# ========== æ˜¾ç¤ºé…ç½® ==========\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“‹ è®­ç»ƒé…ç½®\")\n",
    "print(\"=\"*60)\n",
    "print(f\"è¿è¡Œåç§°: {RUN_NAME}\")\n",
    "print(f\"Episodes: {NUM_EPISODES}\")\n",
    "print(f\"Batch Size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"Learning Rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"Gamma: {TRAINING_CONFIG['gamma']}\")\n",
    "print(f\"Epsilon: {TRAINING_CONFIG['epsilon_start']} â†’ {TRAINING_CONFIG['epsilon_end']}\")\n",
    "print(f\"Hidden Dim: {MODEL_CONFIG['block_hidden_dim']}\")\n",
    "print(f\"\\nä¿å­˜è·¯å¾„: {SAVE_PATH}\")\n",
    "print(f\"æ—¥å¿—è·¯å¾„: {LOG_PATH}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ—¶é—´ä¼°ç®—\n",
    "time_min = NUM_EPISODES * 1\n",
    "time_max = NUM_EPISODES * 3\n",
    "print(f\"\\nâ±ï¸ é¢„ä¼°æ—¶é—´: {time_min}-{time_max} åˆ†é’Ÿ\")\n",
    "print(f\"ğŸ’° æˆæœ¬: $0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. åˆ›å»ºç½‘ç»œå’Œ Agent ğŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"åˆ›å»º DQN Agent...\\n\")\n",
    "\n",
    "# åˆ›å»ºç½‘ç»œ\n",
    "word_vocab_size = len(word2id)\n",
    "online_net = Policy(bert_model, word_vocab_size, MODEL_CONFIG)\n",
    "target_net = Policy(bert_model, word_vocab_size, MODEL_CONFIG)\n",
    "\n",
    "print(f\"âœ“ Policy ç½‘ç»œå·²åˆ›å»º\")\n",
    "print(f\"  å‚æ•°é‡: {sum(p.numel() for p in online_net.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# åˆ›å»º Agent\n",
    "agent = DoubleDQNAgent(\n",
    "    online_net=online_net,\n",
    "    target_net=target_net,\n",
    "    tokenizer=tokenizer,\n",
    "    word2id=word2id,\n",
    "    device=device,\n",
    "    config=TRAINING_CONFIG\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ DoubleDQNAgent å·²åˆ›å»º\")\n",
    "print(f\"  è®¾å¤‡: {device}\")\n",
    "print(f\"  Replay Buffer: {TRAINING_CONFIG['replay_memory_capacity']}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. å¼€å§‹è®­ç»ƒ ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alfworld.agents.environment\n",
    "import yaml\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ å¼€å§‹ DQN è®­ç»ƒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åŠ è½½é…ç½®\n",
    "with open(config_path) as f:\n",
    "    env_config = yaml.safe_load(f)\n",
    "\n",
    "# åˆ›å»ºç¯å¢ƒ\n",
    "print(\"\\n[1/3] åˆ›å»ºè®­ç»ƒç¯å¢ƒ...\")\n",
    "train_env = alfworld.agents.environment.get_environment(env_config[\"env\"][\"type\"])(env_config, train_eval='train')\n",
    "train_env = train_env.init_env(batch_size=1)\n",
    "print(\"âœ“ è®­ç»ƒç¯å¢ƒå·²åˆ›å»º\")\n",
    "\n",
    "print(\"\\n[2/3] åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...\")\n",
    "eval_env = alfworld.agents.environment.get_environment(env_config[\"env\"][\"type\"])(env_config, train_eval='eval_out_of_distribution')\n",
    "eval_env = eval_env.init_env(batch_size=1)\n",
    "print(\"âœ“ è¯„ä¼°ç¯å¢ƒå·²åˆ›å»º\")\n",
    "\n",
    "print(\"\\n[3/3] åˆå§‹åŒ–è®­ç»ƒå¾ªç¯...\")\n",
    "task_desc = \"Your task is to: put some object on some receptacle.\"\n",
    "\n",
    "train_rewards = []\n",
    "train_steps = []\n",
    "train_losses = []\n",
    "eval_rewards = []\n",
    "eval_success_rates = []\n",
    "\n",
    "print(\"âœ“ å‡†å¤‡å®Œæˆ\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå¾ªç¯\n",
    "try:\n",
    "    for episode in tqdm(range(NUM_EPISODES), desc=\"è®­ç»ƒè¿›åº¦\"):\n",
    "        # è®­ç»ƒ\n",
    "        reward, steps, loss, success = run_episode(train_env, agent, task_desc, train=True)\n",
    "        train_rewards.append(reward)\n",
    "        train_steps.append(steps)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # è¯„ä¼°\n",
    "        if (episode + 1) % EVAL_FREQ == 0:\n",
    "            eval_reward_list = []\n",
    "            eval_success_list = []\n",
    "\n",
    "            agent.mode = \"eval\"\n",
    "            for _ in range(5):\n",
    "                eval_reward, _, _, eval_success = run_episode(eval_env, agent, task_desc, train=False)\n",
    "                eval_reward_list.append(eval_reward)\n",
    "                eval_success_list.append(eval_success)\n",
    "            agent.mode = \"train\"\n",
    "\n",
    "            avg_eval_reward = np.mean(eval_reward_list)\n",
    "            avg_eval_success = np.mean(eval_success_list)\n",
    "            eval_rewards.append(avg_eval_reward)\n",
    "            eval_success_rates.append(avg_eval_success)\n",
    "\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Episode {episode+1}/{NUM_EPISODES}\")\n",
    "            print(f\"  è®­ç»ƒå¥–åŠ±: {np.mean(train_rewards[-EVAL_FREQ:]):.3f}\")\n",
    "            print(f\"  è¯„ä¼°å¥–åŠ±: {avg_eval_reward:.3f}\")\n",
    "            print(f\"  è¯„ä¼°æˆåŠŸç‡: {avg_eval_success:.1%}\")\n",
    "            print(f\"  Epsilon: {agent.epsilon:.3f}\")\n",
    "            print(f\"  Loss: {loss:.4f}\")\n",
    "            print(f\"  ç¼“å†²åŒº: {len(agent.memory)}\")\n",
    "            if torch.cuda.is_available():\n",
    "                print(f\"  GPUæ˜¾å­˜: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "        # ä¿å­˜æ£€æŸ¥ç‚¹\n",
    "        if (episode + 1) % SAVE_FREQ == 0:\n",
    "            checkpoint_path = os.path.join(CHECKPOINTS_DIR, f\"{RUN_NAME}_ep{episode+1}.pt\")\n",
    "            torch.save({\n",
    "                'episode': episode + 1,\n",
    "                'online_net': online_net.state_dict(),\n",
    "                'target_net': target_net.state_dict(),\n",
    "                'optimizer': agent.optimizer.state_dict(),\n",
    "                'train_rewards': train_rewards,\n",
    "                'eval_rewards': eval_rewards,\n",
    "                'config': {**MODEL_CONFIG, **TRAINING_CONFIG}\n",
    "            }, checkpoint_path)\n",
    "            print(f\"\\nğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ è®­ç»ƒé”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    torch.save({\n",
    "        'episode': len(train_rewards),\n",
    "        'online_net': online_net.state_dict(),\n",
    "        'target_net': target_net.state_dict(),\n",
    "        'optimizer': agent.optimizer.state_dict(),\n",
    "        'train_rewards': train_rewards,\n",
    "        'eval_rewards': eval_rewards,\n",
    "        'config': {**MODEL_CONFIG, **TRAINING_CONFIG}\n",
    "    }, SAVE_PATH)\n",
    "    print(f\"\\nğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {SAVE_PATH}\")\n",
    "    \n",
    "    train_env.close()\n",
    "    eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. ç»“æœåˆ†æ ğŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ç»Ÿè®¡ä¿¡æ¯\n",
    "print(\"\\nğŸ“Š DQN è®­ç»ƒç»“æœ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"æ€» Episodes: {len(train_rewards)}\")\n",
    "print(f\"å¹³å‡è®­ç»ƒå¥–åŠ±: {np.mean(train_rewards):.3f}\")\n",
    "print(f\"æœ€ç»ˆè®­ç»ƒå¥–åŠ±: {np.mean(train_rewards[-20:]):.3f}\")\n",
    "if eval_rewards:\n",
    "    print(f\"å¹³å‡è¯„ä¼°å¥–åŠ±: {np.mean(eval_rewards):.3f}\")\n",
    "    print(f\"æœ€ç»ˆè¯„ä¼°å¥–åŠ±: {eval_rewards[-1]:.3f}\")\n",
    "if eval_success_rates:\n",
    "    print(f\"å¹³å‡æˆåŠŸç‡: {np.mean(eval_success_rates):.1%}\")\n",
    "    print(f\"æœ€ç»ˆæˆåŠŸç‡: {eval_success_rates[-1]:.1%}\")\n",
    "print(f\"å¹³å‡ Loss: {np.mean([l for l in train_losses if l > 0]):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. è®­ç»ƒå¥–åŠ±\n",
    "axes[0, 0].plot(train_rewards, alpha=0.3, label='Raw', color='#2E86AB')\n",
    "if len(train_rewards) >= 20:\n",
    "    smoothed = pd.Series(train_rewards).rolling(20).mean()\n",
    "    axes[0, 0].plot(smoothed, lw=2, label='Smoothed (20)', color='#2E86AB')\n",
    "axes[0, 0].set_xlabel('Episode', fontsize=11, weight='bold')\n",
    "axes[0, 0].set_ylabel('Reward', fontsize=11, weight='bold')\n",
    "axes[0, 0].set_title('Training Rewards', fontsize=12, weight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. è¯„ä¼°å¥–åŠ±\n",
    "if eval_rewards:\n",
    "    eval_x = list(range(EVAL_FREQ, len(train_rewards)+1, EVAL_FREQ))[:len(eval_rewards)]\n",
    "    axes[0, 1].plot(eval_x, eval_rewards, 'o-', lw=2, ms=6, color='#06A77D')\n",
    "    axes[0, 1].set_xlabel('Episode', fontsize=11, weight='bold')\n",
    "    axes[0, 1].set_ylabel('Avg Reward', fontsize=11, weight='bold')\n",
    "    axes[0, 1].set_title('Evaluation Rewards', fontsize=12, weight='bold')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Loss\n",
    "valid_losses = [(i, l) for i, l in enumerate(train_losses) if l > 0]\n",
    "if valid_losses:\n",
    "    loss_x, loss_y = zip(*valid_losses)\n",
    "    axes[1, 0].plot(loss_x, loss_y, alpha=0.3, color='#A23B72')\n",
    "    if len(loss_y) >= 50:\n",
    "        smoothed_loss = pd.Series(loss_y).rolling(50).mean()\n",
    "        axes[1, 0].plot(loss_x, smoothed_loss, lw=2, color='#A23B72')\n",
    "axes[1, 0].set_xlabel('Episode', fontsize=11, weight='bold')\n",
    "axes[1, 0].set_ylabel('Loss', fontsize=11, weight='bold')\n",
    "axes[1, 0].set_title('DQN Loss', fontsize=12, weight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. æˆåŠŸç‡\n",
    "if eval_success_rates:\n",
    "    eval_x = list(range(EVAL_FREQ, len(train_rewards)+1, EVAL_FREQ))[:len(eval_success_rates)]\n",
    "    axes[1, 1].plot(eval_x, [s*100 for s in eval_success_rates], 'o-', lw=2, ms=6, color='#F18F01')\n",
    "    axes[1, 1].set_xlabel('Episode', fontsize=11, weight='bold')\n",
    "    axes[1, 1].set_ylabel('Success Rate (%)', fontsize=11, weight='bold')\n",
    "    axes[1, 1].set_title('Evaluation Success Rate', fontsize=12, weight='bold')\n",
    "    axes[1, 1].set_ylim([0, 105])\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f'{RUN_NAME}_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ å›¾è¡¨å·²ä¿å­˜: {os.path.join(RESULTS_DIR, f'{RUN_NAME}_curves.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. æµ‹è¯•è®­ç»ƒå¥½çš„ Agent ğŸ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æµ‹è¯•è®­ç»ƒå¥½çš„ Agent...\\n\")\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•ç¯å¢ƒ\n",
    "test_env = alfworld.agents.environment.get_environment(env_config[\"env\"][\"type\"])(env_config, train_eval='eval_out_of_distribution')\n",
    "test_env = test_env.init_env(batch_size=1)\n",
    "\n",
    "# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "agent.mode = \"eval\"\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "print(\"è¿è¡Œ 10 ä¸ªæµ‹è¯• episodes...\\n\")\n",
    "test_results = []\n",
    "\n",
    "for i in range(10):\n",
    "    reward, steps, _, success = run_episode(test_env, agent, task_desc, train=False)\n",
    "    test_results.append({\n",
    "        'episode': i+1,\n",
    "        'reward': reward,\n",
    "        'steps': steps,\n",
    "        'success': success\n",
    "    })\n",
    "    print(f\"Episode {i+1}: Reward={reward:.1f}, Steps={steps}, Success={success}\")\n",
    "\n",
    "test_env.close()\n",
    "\n",
    "# ç»Ÿè®¡\n",
    "df_test = pd.DataFrame(test_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æµ‹è¯•ç»“æœ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"å¹³å‡å¥–åŠ±: {df_test['reward'].mean():.3f}\")\n",
    "print(f\"æˆåŠŸç‡: {df_test['success'].mean():.1%}\")\n",
    "print(f\"å¹³å‡æ­¥æ•°: {df_test['steps'].mean():.1f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. ä¿å­˜ç»“æœåˆ° Drive ğŸ’¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ä¿å­˜è®­ç»ƒå†å²\n",
    "history = {\n",
    "    'train_rewards': train_rewards,\n",
    "    'train_steps': train_steps,\n",
    "    'train_losses': train_losses,\n",
    "    'eval_rewards': eval_rewards,\n",
    "    'eval_success_rates': eval_success_rates,\n",
    "    'config': {\n",
    "        'model': MODEL_CONFIG,\n",
    "        'training': TRAINING_CONFIG,\n",
    "        'num_episodes': NUM_EPISODES\n",
    "    }\n",
    "}\n",
    "\n",
    "history_path = os.path.join(RESULTS_DIR, f'{RUN_NAME}_history.json')\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}\")\n",
    "\n",
    "# åˆ›å»ºæ€»ç»“\n",
    "summary = f\"\"\"\n",
    "# DQN Training Summary\n",
    "\n",
    "## Configuration\n",
    "- Model: DistilBERT + Double DQN\n",
    "- Episodes: {NUM_EPISODES}\n",
    "- Hidden Dim: {MODEL_CONFIG['block_hidden_dim']}\n",
    "- Learning Rate: {TRAINING_CONFIG['learning_rate']}\n",
    "- Batch Size: {TRAINING_CONFIG['batch_size']}\n",
    "- Gamma: {TRAINING_CONFIG['gamma']}\n",
    "\n",
    "## Results\n",
    "- Final Training Reward: {np.mean(train_rewards[-20:]):.3f}\n",
    "- Final Eval Reward: {eval_rewards[-1] if eval_rewards else 'N/A'}\n",
    "- Final Success Rate: {eval_success_rates[-1]:.1%} if eval_success_rates else 'N/A'}\n",
    "- Average Loss: {np.mean([l for l in train_losses if l > 0]):.4f}\n",
    "\n",
    "## Files\n",
    "- Model: {SAVE_PATH}\n",
    "- History: {history_path}\n",
    "- Curves: {os.path.join(RESULTS_DIR, f'{RUN_NAME}_curves.png')}\n",
    "\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "summary_path = os.path.join(RESULTS_DIR, f'{RUN_NAME}_summary.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"âœ“ æ€»ç»“å·²ä¿å­˜: {summary_path}\")\n",
    "print(\"\\n\" + summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. ä¸‹è½½ç»“æœï¼ˆå¯é€‰ï¼‰ â¬‡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    from google.colab import files\n",
    "    \n",
    "    # å‹ç¼©ç»“æœ\n",
    "    zip_path = f\"/content/{RUN_NAME}\"\n",
    "    shutil.make_archive(zip_path, 'zip', RESULTS_DIR)\n",
    "    \n",
    "    print(f\"å‹ç¼©å®Œæˆ: {zip_path}.zip\")\n",
    "    print(\"\\nä¸‹è½½ä¸­...\")\n",
    "    files.download(f\"{zip_path}.zip\")\n",
    "    print(\"âœ“ å®Œæˆ\")\n",
    "else:\n",
    "    print(f\"ç»“æœä½ç½®: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ä½¿ç”¨è¯´æ˜\n",
    "\n",
    "### å¿«é€Ÿå¼€å§‹\n",
    "\n",
    "1. **æ£€æŸ¥ GPU**ï¼šè¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU (T4)\n",
    "2. **è¿è¡Œæ‰€æœ‰ cells**ï¼šé¦–æ¬¡çº¦ 20-30 åˆ†é’Ÿ\n",
    "3. **å¼€å§‹è®­ç»ƒ**ï¼šcell-13 å¼€å§‹è®­ç»ƒå¾ªç¯\n",
    "\n",
    "### è®­ç»ƒå»ºè®®\n",
    "\n",
    "**å¿«é€Ÿæµ‹è¯•ï¼ˆ1-2å°æ—¶ï¼‰**\n",
    "```python\n",
    "NUM_EPISODES = 100\n",
    "EVAL_FREQ = 20\n",
    "```\n",
    "\n",
    "**ä¸­ç­‰è®­ç»ƒï¼ˆåŠå¤©ï¼‰**\n",
    "```python\n",
    "NUM_EPISODES = 500\n",
    "EVAL_FREQ = 50\n",
    "```\n",
    "\n",
    "**å®Œæ•´è®­ç»ƒï¼ˆ1-2å¤©ï¼‰**\n",
    "```python\n",
    "NUM_EPISODES = 2000\n",
    "EVAL_FREQ = 100\n",
    "```\n",
    "\n",
    "### æ¢å¤è®­ç»ƒ\n",
    "\n",
    "```python\n",
    "# åŠ è½½æ£€æŸ¥ç‚¹\n",
    "checkpoint = torch.load('/path/to/checkpoint.pt')\n",
    "online_net.load_state_dict(checkpoint['online_net'])\n",
    "target_net.load_state_dict(checkpoint['target_net'])\n",
    "agent.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "```\n",
    "\n",
    "### æ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "å¦‚æœæ˜¾å­˜ä¸è¶³ï¼š\n",
    "- å‡å° `batch_size`: 64 â†’ 32\n",
    "- å‡å° `replay_memory_capacity`: 100000 â†’ 50000\n",
    "- å‡å° `block_hidden_dim`: 64 â†’ 32\n",
    "\n",
    "### é—®é¢˜æ’æŸ¥\n",
    "\n",
    "**ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥**\n",
    "- æ£€æŸ¥ `ALFWORLD_DATA` ç¯å¢ƒå˜é‡\n",
    "- é‡æ–°è¿è¡Œ cell-4 ä¸‹è½½æ•°æ®\n",
    "\n",
    "**æ˜¾å­˜æº¢å‡º**\n",
    "- å‡å° batch_size\n",
    "- å‡å° replay buffer å¤§å°\n",
    "\n",
    "**è®­ç»ƒä¸æ”¶æ•›**\n",
    "- å¢åŠ è®­ç»ƒ episodes\n",
    "- è°ƒæ•´å­¦ä¹ ç‡\n",
    "- æ£€æŸ¥ epsilon è¡°å‡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
